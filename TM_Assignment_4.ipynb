{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/FabianWalocha/sentimentAnalysis/blob/master/TM_Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tFv7TBoKu3Jx"
   },
   "source": [
    "##CNN-LSTM approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AGgkiERHQvg6"
   },
   "outputs": [],
   "source": [
    "X, y  = [], []\n",
    "with open(\"tweet_data_pre_processed.txt\",'r') as f:\n",
    "  for line in f:\n",
    "    s = line.split(' ')\n",
    "    if s[0]=='positive':\n",
    "      y.append([1,0,0])\n",
    "    elif s[0]=='neutral':\n",
    "      y.append([0,1,0])\n",
    "    elif s[0]=='negative':\n",
    "      y.append([0,0,1])\n",
    "    else:\n",
    "      raise Exception('Error in preprocessing')\n",
    "    X.append(\" \".join(s[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7eWb11ETwsLw"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
    "    \n",
    "    text = re.sub(r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+','<url>',text)\n",
    "    text = re.sub(r'[:;xX8][\\^-]?[)DPp]+','<happy>',text)\n",
    "    text = re.sub(r'\\^[\\.\\_-]?\\^','<happy>',text)\n",
    "    text = re.sub(r':\\'?[-\\^]?[\\/(C]+','<sad>',text)\n",
    "    text = re.sub(r'T[\\._-]?T','<sad>',text)\n",
    "    text = re.sub(r'-[\\._-]?-','<sad>',text)\n",
    "    text = re.sub(r'\\b[oO][\\.-_,]?[oO]\\b','<surpsised>',text)\n",
    "    text = re.sub(r'-[\\._-]?-','<surprised>',text)\n",
    "    text = re.sub(r':[\\^-]?[oO]','<surprised>',text)\n",
    "    text = re.sub(r'D:','<surprised>',text)\n",
    "    \n",
    "    text = re.sub(r\"\\\\\", \"\", text)\n",
    "    text = re.sub(r\"u002c\", \"\", text)\n",
    "    text = re.sub(r\"u2019\", \"'\", text)\n",
    "    text = re.sub(r\"\\n\", \"\",  text)\n",
    "    text = re.sub(r\"[-()]\", \"\", text)\n",
    "    text = re.sub(r\"\\.\", \" .\", text)\n",
    "    text = re.sub(r\"\\!\", \" !\", text)\n",
    "    text = re.sub(r\"\\?\", \" ?\", text)\n",
    "    text = re.sub(r\"\\,\", \" ,\", text)\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    # text = re.sub(r\"he's\", \"he is\", text)\n",
    "    # text = re.sub(r\"she's\", \"she is\", text)\n",
    "    # text = re.sub(r\"it's\", \"it is\", text)\n",
    "    # text = re.sub(r\"that's\", \"that is\", text)\n",
    "    # text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"oh+\", \"oh\", text)\n",
    "    text = re.sub(r\"ah+\", \"ah\", text)\n",
    "    text = re.sub(r\"aa+\",\"a\", text)\n",
    "    text = re.sub(r\"soo+\\b\",'soo', text)\n",
    "    text = re.sub(r\"aint\", \"is not\", text)\n",
    "    text = re.sub(r\"gonna\", \"going to\", text)\n",
    "    text = re.sub(r\"ima\", \"i am going to\", text)\n",
    "    text = re.sub(r\"\\/\\w+\", \"\", text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "kZPuQ4iE2-hv",
    "outputId": "b07acc74-a64a-4917-80d6-6283dbcc1d12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most words in tweet: 60\n",
      "50897 unique tokens in the vocabulary\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from collections import defaultdict\n",
    "\n",
    "Xc = []\n",
    "tweet_lengths = []\n",
    "for line in X:\n",
    "    cleaned = clean_text(line)\n",
    "    Xc.append(cleaned)\n",
    "    tweet_lengths.append(len(cleaned.split(\" \")))\n",
    "X = Xc\n",
    "\n",
    "# inspired by https://github.com/saurabhrathor/InceptionModel_SentimentAnalysis/\n",
    "vocabulary = dict()\n",
    "count = defaultdict(int)\n",
    "inverse_vocabulary = ['PADDING']\n",
    "for text in X:\n",
    "    text = text.split()\n",
    "    for word in text:\n",
    "        count[word]+=1\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = len(inverse_vocabulary)\n",
    "            inverse_vocabulary.append(word)\n",
    "\n",
    "max_len = 0\n",
    "sequences = []\n",
    "for text in X:\n",
    "    text = text.split()\n",
    "    text_sequence = []\n",
    "    for word in text:\n",
    "      # constraint given term frequency\n",
    "        if count[word] >= 1:\n",
    "            text_sequence.append(vocabulary[word])\n",
    "            inverse_vocabulary.append(word)\n",
    "    # save longest tweet length\n",
    "    if len(text_sequence)>max_len:\n",
    "      max_len=len(text_sequence)\n",
    "    sequences.append(text_sequence)\n",
    "print(\"most words in tweet: {}\".format(max(tweet_lengths)))\n",
    "\n",
    "    \n",
    "print(\"%d unique tokens in the vocabulary\" %len(vocabulary))\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = max_len\n",
    "bodies_seq = sequence.pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "4UC6ZRE5-ofO",
    "outputId": "80813d96-d84d-40db-e51b-ff5715a90a7e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "w2v = Word2Vec([\" \".join(X).split(\" \")],size=200, window=2, min_count=1, workers=4)\n",
    "# w2v.train(X, total_examples=w2v.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "bodies_seq = np.zeros([len(X),max(tweet_lengths),200])\n",
    "for idx,tweet in enumerate(X):\n",
    "  for idx2,word in enumerate(tweet.split(\" \")):\n",
    "    bodies_seq[idx,idx2,:] = w2v[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "FvY6Zpob-3Qp",
    "outputId": "ae2e8c7b-5014-456c-9bb6-f1330994dc64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 45300 examples\n",
      "Val: 4530 examples\n",
      "Test: 504 examples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(np.array(bodies_seq), np.array(y), test_size=0.1)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.1)\n",
    "\n",
    "print('Train: {} examples'.format(len(y_train)))\n",
    "print('Val: {} examples'.format(len(y_val)))\n",
    "print('Test: {} examples'.format(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XKDZYA05AdDk"
   },
   "outputs": [],
   "source": [
    "# inspired by https://github.com/mihirahlawat/Sentiment-Analysis\n",
    "# BB_twtr at SemEval-2017 Task 4: Twitter Sentiment Analysis with CNNs and LSTMs\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Flatten, Conv1D, SpatialDropout1D, MaxPooling1D, AveragePooling1D, Bidirectional, merge, concatenate, Input, Dropout, LSTM\n",
    "\n",
    "def w2vmodel(embedding_size=200, max_words=200, y_dim=1, vocabulary_size=50,\n",
    "          num_filters=200, filter_sizes=[3,4,5], pool_padding='valid', dropout=0.5):\n",
    "    embed_input = Input(shape=(max_words,embedding_size))\n",
    "    pooled_outputs = []\n",
    "    for i in range(len(filter_sizes)):\n",
    "        conv = Conv1D(num_filters, kernel_size=filter_sizes[i], padding='valid', activation='relu')(embed_input)\n",
    "        conv = MaxPooling1D(pool_size=max_words-filter_sizes[i]+1)(conv)           \n",
    "        pooled_outputs.append(conv)\n",
    "    merge = concatenate(pooled_outputs)\n",
    "    \n",
    "    x = Dense(30, activation='relu')(merge)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Bidirectional(LSTM(100, return_sequences=True, dropout=0.5, recurrent_dropout=0.1))(x)\n",
    "    x = Dense(30, activation='relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(y_dim, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=embed_input,outputs=x)\n",
    "\n",
    "    # model.compile(optimizer='adam',loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "    # print(model.summary())\n",
    "    \n",
    "    from keras.utils import plot_model\n",
    "    plot_model(model, to_file='shared_input_layer.png')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_3fZjhvuub-F"
   },
   "outputs": [],
   "source": [
    "# inspired by https://github.com/mihirahlawat/Sentiment-Analysis\n",
    "# BB_twtr at SemEval-2017 Task 4: Twitter Sentiment Analysis with CNNs and LSTMs\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import Flatten, Conv1D, SpatialDropout1D, MaxPooling1D, AveragePooling1D, Bidirectional, merge, concatenate, Input, Dropout, LSTM\n",
    "\n",
    "def model(embedding_size=200, max_words=200, y_dim=1, vocabulary_size=50,\n",
    "          num_filters=200, filter_sizes=[3,4,5], pool_padding='valid', dropout=0.5):\n",
    "    embed_input = Input(shape=(max_words,))\n",
    "    x = Embedding(vocabulary_size, embedding_size, input_length=max_words)(embed_input)\n",
    "    print(x)\n",
    "    pooled_outputs = []\n",
    "    for i in range(len(filter_sizes)):\n",
    "        conv = Conv1D(num_filters, kernel_size=filter_sizes[i], padding='valid', activation='relu')(x)\n",
    "        conv = MaxPooling1D(pool_size=max_words-filter_sizes[i]+1)(conv)           \n",
    "        pooled_outputs.append(conv)\n",
    "    merge = concatenate(pooled_outputs)\n",
    "    \n",
    "    x = Dense(30, activation='relu')(merge)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Bidirectional(LSTM(100, return_sequences=True, dropout=0.5, recurrent_dropout=0.1))(x)\n",
    "    x = Dense(30, activation='relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(y_dim, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=embed_input,outputs=x)\n",
    "\n",
    "    # model.compile(optimizer='adam',loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "    # print(model.summary())\n",
    "    \n",
    "    from keras.utils import plot_model\n",
    "    plot_model(model, to_file='shared_input_layer.png')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yIJSeKoN7M9k"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ynuctgxl_g1k",
    "outputId": "ca7de54c-abd7-4919-9c4c-f0e309974a6d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1335: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45300 samples, validate on 4530 samples\n",
      "Epoch 1/200\n",
      "45300/45300 [==============================] - 34s 760us/step - loss: 0.6583 - acc: 0.4365 - f1_m: 0.2836 - precision_m: 0.4183 - recall_m: 0.2516 - val_loss: 0.6444 - val_acc: 0.4369 - val_f1_m: 0.4350 - val_precision_m: 0.4434 - val_recall_m: 0.4272\n",
      "Epoch 2/200\n",
      "45300/45300 [==============================] - 31s 693us/step - loss: 0.6112 - acc: 0.4297 - f1_m: 0.4184 - precision_m: 0.4429 - recall_m: 0.3985 - val_loss: 0.5937 - val_acc: 0.4272 - val_f1_m: 0.4234 - val_precision_m: 0.4300 - val_recall_m: 0.4172\n",
      "Epoch 3/200\n",
      "45300/45300 [==============================] - 32s 696us/step - loss: 0.5764 - acc: 0.4682 - f1_m: 0.4658 - precision_m: 0.4706 - recall_m: 0.4613 - val_loss: 0.5611 - val_acc: 0.4960 - val_f1_m: 0.4955 - val_precision_m: 0.4966 - val_recall_m: 0.4945\n",
      "Epoch 4/200\n",
      "45300/45300 [==============================] - 31s 691us/step - loss: 0.5539 - acc: 0.4933 - f1_m: 0.4923 - precision_m: 0.4942 - recall_m: 0.4905 - val_loss: 0.5480 - val_acc: 0.5132 - val_f1_m: 0.5131 - val_precision_m: 0.5131 - val_recall_m: 0.5130\n",
      "Epoch 5/200\n",
      "45300/45300 [==============================] - 31s 691us/step - loss: 0.5292 - acc: 0.5209 - f1_m: 0.5206 - precision_m: 0.5210 - recall_m: 0.5202 - val_loss: 0.5328 - val_acc: 0.5168 - val_f1_m: 0.5168 - val_precision_m: 0.5168 - val_recall_m: 0.5168\n",
      "Epoch 6/200\n",
      "45300/45300 [==============================] - 32s 702us/step - loss: 0.5135 - acc: 0.5384 - f1_m: 0.5385 - precision_m: 0.5388 - recall_m: 0.5381 - val_loss: 0.5519 - val_acc: 0.4748 - val_f1_m: 0.4744 - val_precision_m: 0.4748 - val_recall_m: 0.4740\n",
      "Epoch 7/200\n",
      "45300/45300 [==============================] - 31s 687us/step - loss: 0.5048 - acc: 0.5467 - f1_m: 0.5464 - precision_m: 0.5467 - recall_m: 0.5461 - val_loss: 0.5515 - val_acc: 0.4870 - val_f1_m: 0.4870 - val_precision_m: 0.4871 - val_recall_m: 0.4870\n",
      "Epoch 8/200\n",
      "45300/45300 [==============================] - 31s 695us/step - loss: 0.4906 - acc: 0.5591 - f1_m: 0.5589 - precision_m: 0.5592 - recall_m: 0.5586 - val_loss: 0.5406 - val_acc: 0.5009 - val_f1_m: 0.5001 - val_precision_m: 0.5009 - val_recall_m: 0.4993\n",
      "Epoch 9/200\n",
      "45300/45300 [==============================] - 31s 679us/step - loss: 0.4872 - acc: 0.5641 - f1_m: 0.5641 - precision_m: 0.5642 - recall_m: 0.5640 - val_loss: 0.5218 - val_acc: 0.5563 - val_f1_m: 0.5563 - val_precision_m: 0.5563 - val_recall_m: 0.5563\n",
      "Epoch 10/200\n",
      "45300/45300 [==============================] - 32s 696us/step - loss: 0.4856 - acc: 0.5700 - f1_m: 0.5700 - precision_m: 0.5702 - recall_m: 0.5698 - val_loss: 0.5294 - val_acc: 0.5119 - val_f1_m: 0.5120 - val_precision_m: 0.5124 - val_recall_m: 0.5117\n",
      "Epoch 11/200\n",
      "45300/45300 [==============================] - 31s 690us/step - loss: 0.4746 - acc: 0.5783 - f1_m: 0.5782 - precision_m: 0.5785 - recall_m: 0.5780 - val_loss: 0.5114 - val_acc: 0.5439 - val_f1_m: 0.5433 - val_precision_m: 0.5436 - val_recall_m: 0.5430\n",
      "Epoch 12/200\n",
      "45300/45300 [==============================] - 32s 698us/step - loss: 0.4729 - acc: 0.5811 - f1_m: 0.5812 - precision_m: 0.5814 - recall_m: 0.5810 - val_loss: 0.5819 - val_acc: 0.5667 - val_f1_m: 0.5667 - val_precision_m: 0.5667 - val_recall_m: 0.5667\n",
      "Epoch 13/200\n",
      "45300/45300 [==============================] - 31s 689us/step - loss: 0.4669 - acc: 0.5849 - f1_m: 0.5847 - precision_m: 0.5849 - recall_m: 0.5845 - val_loss: 0.5107 - val_acc: 0.5490 - val_f1_m: 0.5490 - val_precision_m: 0.5491 - val_recall_m: 0.5488\n",
      "Epoch 14/200\n",
      "45300/45300 [==============================] - 31s 677us/step - loss: 0.4666 - acc: 0.5881 - f1_m: 0.5880 - precision_m: 0.5882 - recall_m: 0.5879 - val_loss: 0.5038 - val_acc: 0.5570 - val_f1_m: 0.5570 - val_precision_m: 0.5571 - val_recall_m: 0.5570\n",
      "Epoch 15/200\n",
      "45300/45300 [==============================] - 31s 685us/step - loss: 0.4556 - acc: 0.5949 - f1_m: 0.5948 - precision_m: 0.5950 - recall_m: 0.5947 - val_loss: 0.5063 - val_acc: 0.5614 - val_f1_m: 0.5614 - val_precision_m: 0.5617 - val_recall_m: 0.5611\n",
      "Epoch 16/200\n",
      "45300/45300 [==============================] - 31s 693us/step - loss: 0.4606 - acc: 0.5943 - f1_m: 0.5943 - precision_m: 0.5945 - recall_m: 0.5942 - val_loss: 0.5126 - val_acc: 0.5539 - val_f1_m: 0.5537 - val_precision_m: 0.5538 - val_recall_m: 0.5536\n",
      "Epoch 17/200\n",
      "45300/45300 [==============================] - 30s 671us/step - loss: 0.4582 - acc: 0.5966 - f1_m: 0.5965 - precision_m: 0.5966 - recall_m: 0.5964 - val_loss: 0.5092 - val_acc: 0.5709 - val_f1_m: 0.5709 - val_precision_m: 0.5709 - val_recall_m: 0.5709\n",
      "Epoch 18/200\n",
      "45300/45300 [==============================] - 31s 680us/step - loss: 0.4661 - acc: 0.5867 - f1_m: 0.5866 - precision_m: 0.5867 - recall_m: 0.5866 - val_loss: 0.5027 - val_acc: 0.5702 - val_f1_m: 0.5703 - val_precision_m: 0.5703 - val_recall_m: 0.5702\n",
      "Epoch 19/200\n",
      "45300/45300 [==============================] - 31s 684us/step - loss: 0.4524 - acc: 0.5988 - f1_m: 0.5988 - precision_m: 0.5988 - recall_m: 0.5987 - val_loss: 0.5418 - val_acc: 0.5726 - val_f1_m: 0.5726 - val_precision_m: 0.5726 - val_recall_m: 0.5726\n",
      "Epoch 20/200\n",
      "45300/45300 [==============================] - 31s 694us/step - loss: 0.4502 - acc: 0.6046 - f1_m: 0.6046 - precision_m: 0.6047 - recall_m: 0.6046 - val_loss: 0.5413 - val_acc: 0.5543 - val_f1_m: 0.5544 - val_precision_m: 0.5544 - val_recall_m: 0.5543\n",
      "Epoch 21/200\n",
      "45300/45300 [==============================] - 32s 704us/step - loss: 0.4505 - acc: 0.5999 - f1_m: 0.5998 - precision_m: 0.5999 - recall_m: 0.5998 - val_loss: 0.5022 - val_acc: 0.5514 - val_f1_m: 0.5515 - val_precision_m: 0.5515 - val_recall_m: 0.5514\n",
      "Epoch 22/200\n",
      "45300/45300 [==============================] - 31s 690us/step - loss: 0.4485 - acc: 0.6030 - f1_m: 0.6030 - precision_m: 0.6030 - recall_m: 0.6030 - val_loss: 0.5167 - val_acc: 0.5362 - val_f1_m: 0.5363 - val_precision_m: 0.5364 - val_recall_m: 0.5362\n",
      "Epoch 23/200\n",
      "45300/45300 [==============================] - 31s 683us/step - loss: 0.4432 - acc: 0.6089 - f1_m: 0.6089 - precision_m: 0.6089 - recall_m: 0.6089 - val_loss: 0.5119 - val_acc: 0.5475 - val_f1_m: 0.5476 - val_precision_m: 0.5477 - val_recall_m: 0.5475\n",
      "Epoch 24/200\n",
      "45300/45300 [==============================] - 31s 683us/step - loss: 0.4777 - acc: 0.6036 - f1_m: 0.6036 - precision_m: 0.6036 - recall_m: 0.6035 - val_loss: 0.5018 - val_acc: 0.5640 - val_f1_m: 0.5640 - val_precision_m: 0.5640 - val_recall_m: 0.5640\n",
      "Epoch 25/200\n",
      "45300/45300 [==============================] - 30s 662us/step - loss: 0.4604 - acc: 0.5992 - f1_m: 0.5991 - precision_m: 0.5992 - recall_m: 0.5991 - val_loss: 0.5161 - val_acc: 0.5252 - val_f1_m: 0.5251 - val_precision_m: 0.5252 - val_recall_m: 0.5249\n",
      "Epoch 26/200\n",
      "45300/45300 [==============================] - 31s 695us/step - loss: 0.4445 - acc: 0.6063 - f1_m: 0.6063 - precision_m: 0.6063 - recall_m: 0.6062 - val_loss: 0.4990 - val_acc: 0.5647 - val_f1_m: 0.5647 - val_precision_m: 0.5647 - val_recall_m: 0.5647\n",
      "Epoch 27/200\n",
      "45300/45300 [==============================] - 31s 690us/step - loss: 0.4387 - acc: 0.6151 - f1_m: 0.6151 - precision_m: 0.6152 - recall_m: 0.6151 - val_loss: 0.5037 - val_acc: 0.5795 - val_f1_m: 0.5795 - val_precision_m: 0.5796 - val_recall_m: 0.5795\n",
      "Epoch 28/200\n",
      "45300/45300 [==============================] - 31s 686us/step - loss: 0.4451 - acc: 0.6014 - f1_m: 0.6014 - precision_m: 0.6014 - recall_m: 0.6013 - val_loss: 0.5265 - val_acc: 0.5733 - val_f1_m: 0.5734 - val_precision_m: 0.5734 - val_recall_m: 0.5733\n",
      "Epoch 29/200\n",
      "45300/45300 [==============================] - 31s 675us/step - loss: 0.4663 - acc: 0.6048 - f1_m: 0.6048 - precision_m: 0.6048 - recall_m: 0.6048 - val_loss: 0.5064 - val_acc: 0.5651 - val_f1_m: 0.5651 - val_precision_m: 0.5651 - val_recall_m: 0.5651\n",
      "Epoch 30/200\n",
      "45300/45300 [==============================] - 31s 685us/step - loss: 0.4328 - acc: 0.6191 - f1_m: 0.6190 - precision_m: 0.6191 - recall_m: 0.6190 - val_loss: 0.5594 - val_acc: 0.5726 - val_f1_m: 0.5726 - val_precision_m: 0.5726 - val_recall_m: 0.5726\n",
      "Epoch 31/200\n",
      "45300/45300 [==============================] - 31s 682us/step - loss: 0.4378 - acc: 0.6174 - f1_m: 0.6173 - precision_m: 0.6174 - recall_m: 0.6173 - val_loss: 0.4989 - val_acc: 0.5773 - val_f1_m: 0.5773 - val_precision_m: 0.5774 - val_recall_m: 0.5773\n",
      "Epoch 32/200\n",
      "45300/45300 [==============================] - 31s 677us/step - loss: 0.4367 - acc: 0.6160 - f1_m: 0.6160 - precision_m: 0.6161 - recall_m: 0.6160 - val_loss: 0.5736 - val_acc: 0.5693 - val_f1_m: 0.5693 - val_precision_m: 0.5693 - val_recall_m: 0.5693\n",
      "Epoch 33/200\n",
      "45300/45300 [==============================] - 30s 672us/step - loss: 0.4379 - acc: 0.6150 - f1_m: 0.6150 - precision_m: 0.6150 - recall_m: 0.6150 - val_loss: 0.5046 - val_acc: 0.5589 - val_f1_m: 0.5590 - val_precision_m: 0.5590 - val_recall_m: 0.5589\n",
      "Epoch 34/200\n",
      "45300/45300 [==============================] - 30s 669us/step - loss: 0.4297 - acc: 0.6188 - f1_m: 0.6189 - precision_m: 0.6189 - recall_m: 0.6188 - val_loss: 0.4891 - val_acc: 0.5713 - val_f1_m: 0.5714 - val_precision_m: 0.5714 - val_recall_m: 0.5713\n",
      "Epoch 35/200\n",
      "45300/45300 [==============================] - 30s 671us/step - loss: 0.4357 - acc: 0.6132 - f1_m: 0.6132 - precision_m: 0.6133 - recall_m: 0.6132 - val_loss: 0.4943 - val_acc: 0.5550 - val_f1_m: 0.5551 - val_precision_m: 0.5553 - val_recall_m: 0.5550\n",
      "Epoch 36/200\n",
      "45300/45300 [==============================] - 31s 681us/step - loss: 0.4358 - acc: 0.6185 - f1_m: 0.6185 - precision_m: 0.6186 - recall_m: 0.6185 - val_loss: 0.4989 - val_acc: 0.5596 - val_f1_m: 0.5597 - val_precision_m: 0.5597 - val_recall_m: 0.5596\n",
      "Epoch 37/200\n",
      "45300/45300 [==============================] - 31s 685us/step - loss: 0.4282 - acc: 0.6226 - f1_m: 0.6225 - precision_m: 0.6226 - recall_m: 0.6225 - val_loss: 0.5305 - val_acc: 0.5514 - val_f1_m: 0.5513 - val_precision_m: 0.5513 - val_recall_m: 0.5512\n",
      "Epoch 38/200\n",
      "45300/45300 [==============================] - 31s 694us/step - loss: 0.4325 - acc: 0.6176 - f1_m: 0.6176 - precision_m: 0.6176 - recall_m: 0.6175 - val_loss: 0.5041 - val_acc: 0.5594 - val_f1_m: 0.5594 - val_precision_m: 0.5594 - val_recall_m: 0.5594\n",
      "Epoch 39/200\n",
      "45300/45300 [==============================] - 30s 662us/step - loss: 0.4382 - acc: 0.6132 - f1_m: 0.6132 - precision_m: 0.6132 - recall_m: 0.6132 - val_loss: 0.5116 - val_acc: 0.5439 - val_f1_m: 0.5438 - val_precision_m: 0.5440 - val_recall_m: 0.5437\n",
      "Epoch 40/200\n",
      "45300/45300 [==============================] - 30s 660us/step - loss: 0.4370 - acc: 0.6182 - f1_m: 0.6182 - precision_m: 0.6182 - recall_m: 0.6182 - val_loss: 0.4870 - val_acc: 0.5658 - val_f1_m: 0.5659 - val_precision_m: 0.5659 - val_recall_m: 0.5658\n",
      "Epoch 41/200\n",
      "45300/45300 [==============================] - 31s 677us/step - loss: 0.4280 - acc: 0.6218 - f1_m: 0.6218 - precision_m: 0.6218 - recall_m: 0.6218 - val_loss: 0.4964 - val_acc: 0.5594 - val_f1_m: 0.5592 - val_precision_m: 0.5593 - val_recall_m: 0.5592\n",
      "Epoch 42/200\n",
      "45300/45300 [==============================] - 31s 678us/step - loss: 0.4303 - acc: 0.6233 - f1_m: 0.6233 - precision_m: 0.6234 - recall_m: 0.6233 - val_loss: 0.5773 - val_acc: 0.5766 - val_f1_m: 0.5766 - val_precision_m: 0.5766 - val_recall_m: 0.5766\n",
      "Epoch 43/200\n",
      "45300/45300 [==============================] - 31s 690us/step - loss: 0.4315 - acc: 0.6199 - f1_m: 0.6200 - precision_m: 0.6200 - recall_m: 0.6199 - val_loss: 0.5055 - val_acc: 0.5717 - val_f1_m: 0.5717 - val_precision_m: 0.5717 - val_recall_m: 0.5717\n",
      "Epoch 44/200\n",
      "45300/45300 [==============================] - 31s 676us/step - loss: 0.4322 - acc: 0.6208 - f1_m: 0.6208 - precision_m: 0.6208 - recall_m: 0.6208 - val_loss: 0.5204 - val_acc: 0.5678 - val_f1_m: 0.5678 - val_precision_m: 0.5678 - val_recall_m: 0.5678\n",
      "Epoch 45/200\n",
      "45300/45300 [==============================] - 31s 692us/step - loss: 0.4314 - acc: 0.6152 - f1_m: 0.6152 - precision_m: 0.6152 - recall_m: 0.6152 - val_loss: 0.5006 - val_acc: 0.5722 - val_f1_m: 0.5722 - val_precision_m: 0.5722 - val_recall_m: 0.5722\n",
      "Epoch 46/200\n",
      "45300/45300 [==============================] - 31s 687us/step - loss: 0.4340 - acc: 0.6203 - f1_m: 0.6203 - precision_m: 0.6203 - recall_m: 0.6202 - val_loss: 0.5244 - val_acc: 0.5923 - val_f1_m: 0.5923 - val_precision_m: 0.5923 - val_recall_m: 0.5923\n",
      "Epoch 47/200\n",
      "45300/45300 [==============================] - 31s 685us/step - loss: 0.4365 - acc: 0.6169 - f1_m: 0.6169 - precision_m: 0.6169 - recall_m: 0.6169 - val_loss: 0.5740 - val_acc: 0.5693 - val_f1_m: 0.5693 - val_precision_m: 0.5693 - val_recall_m: 0.5693\n",
      "Epoch 48/200\n",
      "45300/45300 [==============================] - 31s 682us/step - loss: 0.4360 - acc: 0.6159 - f1_m: 0.6159 - precision_m: 0.6159 - recall_m: 0.6159 - val_loss: 0.4928 - val_acc: 0.5664 - val_f1_m: 0.5663 - val_precision_m: 0.5664 - val_recall_m: 0.5662\n",
      "Epoch 49/200\n",
      "45300/45300 [==============================] - 30s 667us/step - loss: 0.4285 - acc: 0.6243 - f1_m: 0.6243 - precision_m: 0.6243 - recall_m: 0.6243 - val_loss: 0.5066 - val_acc: 0.5331 - val_f1_m: 0.5331 - val_precision_m: 0.5331 - val_recall_m: 0.5331\n",
      "Epoch 50/200\n",
      "45300/45300 [==============================] - 31s 685us/step - loss: 0.4315 - acc: 0.6215 - f1_m: 0.6215 - precision_m: 0.6215 - recall_m: 0.6215 - val_loss: 0.5561 - val_acc: 0.5810 - val_f1_m: 0.5810 - val_precision_m: 0.5810 - val_recall_m: 0.5810\n",
      "Epoch 51/200\n",
      "45300/45300 [==============================] - 31s 683us/step - loss: 0.4378 - acc: 0.6150 - f1_m: 0.6150 - precision_m: 0.6150 - recall_m: 0.6150 - val_loss: 0.4968 - val_acc: 0.5558 - val_f1_m: 0.5559 - val_precision_m: 0.5560 - val_recall_m: 0.5558\n",
      "Epoch 52/200\n",
      "45300/45300 [==============================] - 31s 688us/step - loss: 0.4431 - acc: 0.6239 - f1_m: 0.6238 - precision_m: 0.6239 - recall_m: 0.6238 - val_loss: 0.4856 - val_acc: 0.5711 - val_f1_m: 0.5712 - val_precision_m: 0.5712 - val_recall_m: 0.5711\n",
      "Epoch 53/200\n",
      "45300/45300 [==============================] - 31s 687us/step - loss: 0.4237 - acc: 0.6260 - f1_m: 0.6260 - precision_m: 0.6260 - recall_m: 0.6259 - val_loss: 0.4964 - val_acc: 0.5722 - val_f1_m: 0.5722 - val_precision_m: 0.5722 - val_recall_m: 0.5722\n",
      "Epoch 54/200\n",
      "45300/45300 [==============================] - 31s 681us/step - loss: 0.4354 - acc: 0.6237 - f1_m: 0.6237 - precision_m: 0.6238 - recall_m: 0.6237 - val_loss: 0.5043 - val_acc: 0.5417 - val_f1_m: 0.5417 - val_precision_m: 0.5417 - val_recall_m: 0.5417\n",
      "Epoch 55/200\n",
      "45300/45300 [==============================] - 31s 685us/step - loss: 0.4317 - acc: 0.6188 - f1_m: 0.6188 - precision_m: 0.6188 - recall_m: 0.6188 - val_loss: 0.5258 - val_acc: 0.5596 - val_f1_m: 0.5596 - val_precision_m: 0.5596 - val_recall_m: 0.5596\n",
      "Epoch 56/200\n",
      "45300/45300 [==============================] - 31s 678us/step - loss: 0.4449 - acc: 0.6135 - f1_m: 0.6135 - precision_m: 0.6135 - recall_m: 0.6135 - val_loss: 0.5171 - val_acc: 0.5744 - val_f1_m: 0.5744 - val_precision_m: 0.5744 - val_recall_m: 0.5744\n",
      "Epoch 57/200\n",
      "45300/45300 [==============================] - 31s 684us/step - loss: 0.4386 - acc: 0.6126 - f1_m: 0.6126 - precision_m: 0.6126 - recall_m: 0.6126 - val_loss: 0.5016 - val_acc: 0.5704 - val_f1_m: 0.5704 - val_precision_m: 0.5704 - val_recall_m: 0.5704\n",
      "Epoch 58/200\n",
      "45300/45300 [==============================] - 31s 677us/step - loss: 0.4281 - acc: 0.6225 - f1_m: 0.6225 - precision_m: 0.6225 - recall_m: 0.6225 - val_loss: 0.4984 - val_acc: 0.5391 - val_f1_m: 0.5391 - val_precision_m: 0.5392 - val_recall_m: 0.5391\n",
      "Epoch 59/200\n",
      "45300/45300 [==============================] - 31s 677us/step - loss: 0.4327 - acc: 0.6200 - f1_m: 0.6200 - precision_m: 0.6200 - recall_m: 0.6199 - val_loss: 0.6143 - val_acc: 0.4199 - val_f1_m: 0.4199 - val_precision_m: 0.4199 - val_recall_m: 0.4199\n",
      "Epoch 60/200\n",
      "45300/45300 [==============================] - 31s 678us/step - loss: 0.4615 - acc: 0.5969 - f1_m: 0.5969 - precision_m: 0.5969 - recall_m: 0.5969 - val_loss: 0.5282 - val_acc: 0.5687 - val_f1_m: 0.5687 - val_precision_m: 0.5687 - val_recall_m: 0.5687\n",
      "Epoch 61/200\n",
      "45300/45300 [==============================] - 31s 680us/step - loss: 0.4400 - acc: 0.6127 - f1_m: 0.6127 - precision_m: 0.6128 - recall_m: 0.6127 - val_loss: 0.5123 - val_acc: 0.5556 - val_f1_m: 0.5556 - val_precision_m: 0.5556 - val_recall_m: 0.5556\n",
      "Epoch 62/200\n",
      "45300/45300 [==============================] - 31s 685us/step - loss: 0.4364 - acc: 0.6182 - f1_m: 0.6182 - precision_m: 0.6182 - recall_m: 0.6182 - val_loss: 0.5852 - val_acc: 0.5097 - val_f1_m: 0.5098 - val_precision_m: 0.5098 - val_recall_m: 0.5097\n",
      "Epoch 63/200\n",
      "45300/45300 [==============================] - 31s 676us/step - loss: 0.4427 - acc: 0.6071 - f1_m: 0.6071 - precision_m: 0.6071 - recall_m: 0.6071 - val_loss: 0.5002 - val_acc: 0.5512 - val_f1_m: 0.5512 - val_precision_m: 0.5512 - val_recall_m: 0.5512\n",
      "Epoch 64/200\n",
      "45300/45300 [==============================] - 31s 677us/step - loss: 0.4379 - acc: 0.6177 - f1_m: 0.6177 - precision_m: 0.6177 - recall_m: 0.6177 - val_loss: 0.5042 - val_acc: 0.5711 - val_f1_m: 0.5711 - val_precision_m: 0.5711 - val_recall_m: 0.5711\n",
      "Epoch 65/200\n",
      "45300/45300 [==============================] - 31s 679us/step - loss: 0.4336 - acc: 0.6176 - f1_m: 0.6176 - precision_m: 0.6176 - recall_m: 0.6176 - val_loss: 0.4934 - val_acc: 0.5775 - val_f1_m: 0.5775 - val_precision_m: 0.5775 - val_recall_m: 0.5775\n",
      "Epoch 66/200\n",
      "45300/45300 [==============================] - 31s 679us/step - loss: 0.4207 - acc: 0.6280 - f1_m: 0.6280 - precision_m: 0.6280 - recall_m: 0.6280 - val_loss: 0.4860 - val_acc: 0.5682 - val_f1_m: 0.5682 - val_precision_m: 0.5682 - val_recall_m: 0.5682\n",
      "Epoch 67/200\n",
      "45300/45300 [==============================] - 31s 677us/step - loss: 0.4231 - acc: 0.6266 - f1_m: 0.6266 - precision_m: 0.6266 - recall_m: 0.6266 - val_loss: 0.4828 - val_acc: 0.5698 - val_f1_m: 0.5698 - val_precision_m: 0.5698 - val_recall_m: 0.5698\n",
      "Epoch 68/200\n",
      "45300/45300 [==============================] - 31s 679us/step - loss: 0.4305 - acc: 0.6263 - f1_m: 0.6263 - precision_m: 0.6263 - recall_m: 0.6263 - val_loss: 0.4919 - val_acc: 0.5494 - val_f1_m: 0.5494 - val_precision_m: 0.5494 - val_recall_m: 0.5494\n",
      "Epoch 69/200\n",
      "45300/45300 [==============================] - 31s 683us/step - loss: 0.4376 - acc: 0.6230 - f1_m: 0.6229 - precision_m: 0.6229 - recall_m: 0.6229 - val_loss: 0.5276 - val_acc: 0.5702 - val_f1_m: 0.5702 - val_precision_m: 0.5702 - val_recall_m: 0.5702\n",
      "Epoch 70/200\n",
      "45300/45300 [==============================] - 31s 687us/step - loss: 0.4331 - acc: 0.6234 - f1_m: 0.6233 - precision_m: 0.6233 - recall_m: 0.6233 - val_loss: 0.4764 - val_acc: 0.5775 - val_f1_m: 0.5775 - val_precision_m: 0.5775 - val_recall_m: 0.5775\n",
      "Epoch 71/200\n",
      "45300/45300 [==============================] - 31s 687us/step - loss: 0.4298 - acc: 0.6267 - f1_m: 0.6268 - precision_m: 0.6268 - recall_m: 0.6267 - val_loss: 0.5005 - val_acc: 0.5490 - val_f1_m: 0.5490 - val_precision_m: 0.5490 - val_recall_m: 0.5490\n",
      "Epoch 72/200\n",
      "45300/45300 [==============================] - 31s 689us/step - loss: 0.4444 - acc: 0.6065 - f1_m: 0.6065 - precision_m: 0.6065 - recall_m: 0.6065 - val_loss: 0.5101 - val_acc: 0.5695 - val_f1_m: 0.5695 - val_precision_m: 0.5695 - val_recall_m: 0.5695\n",
      "Epoch 73/200\n",
      "45300/45300 [==============================] - 30s 672us/step - loss: 0.4355 - acc: 0.6216 - f1_m: 0.6216 - precision_m: 0.6216 - recall_m: 0.6215 - val_loss: 0.4812 - val_acc: 0.5671 - val_f1_m: 0.5671 - val_precision_m: 0.5671 - val_recall_m: 0.5671\n",
      "Epoch 74/200\n",
      "45300/45300 [==============================] - 31s 678us/step - loss: 0.4245 - acc: 0.6231 - f1_m: 0.6231 - precision_m: 0.6231 - recall_m: 0.6231 - val_loss: 0.5405 - val_acc: 0.5631 - val_f1_m: 0.5631 - val_precision_m: 0.5631 - val_recall_m: 0.5631\n",
      "Epoch 75/200\n",
      "45300/45300 [==============================] - 30s 670us/step - loss: 0.4287 - acc: 0.6180 - f1_m: 0.6180 - precision_m: 0.6180 - recall_m: 0.6180 - val_loss: 0.4804 - val_acc: 0.5667 - val_f1_m: 0.5667 - val_precision_m: 0.5667 - val_recall_m: 0.5667\n",
      "Epoch 76/200\n",
      "45300/45300 [==============================] - 31s 678us/step - loss: 0.4309 - acc: 0.6234 - f1_m: 0.6234 - precision_m: 0.6235 - recall_m: 0.6234 - val_loss: 0.4777 - val_acc: 0.5779 - val_f1_m: 0.5780 - val_precision_m: 0.5781 - val_recall_m: 0.5779\n",
      "Epoch 77/200\n",
      "45300/45300 [==============================] - 32s 702us/step - loss: 0.4244 - acc: 0.6299 - f1_m: 0.6299 - precision_m: 0.6299 - recall_m: 0.6299 - val_loss: 0.4771 - val_acc: 0.5742 - val_f1_m: 0.5742 - val_precision_m: 0.5742 - val_recall_m: 0.5742\n",
      "Epoch 78/200\n",
      "45300/45300 [==============================] - 31s 689us/step - loss: 0.4268 - acc: 0.6245 - f1_m: 0.6245 - precision_m: 0.6245 - recall_m: 0.6245 - val_loss: 0.5416 - val_acc: 0.5682 - val_f1_m: 0.5682 - val_precision_m: 0.5682 - val_recall_m: 0.5682\n",
      "Epoch 79/200\n",
      "45300/45300 [==============================] - 31s 684us/step - loss: 0.4284 - acc: 0.6266 - f1_m: 0.6266 - precision_m: 0.6266 - recall_m: 0.6266 - val_loss: 0.5003 - val_acc: 0.5792 - val_f1_m: 0.5792 - val_precision_m: 0.5792 - val_recall_m: 0.5792\n",
      "Epoch 80/200\n",
      "45300/45300 [==============================] - 30s 669us/step - loss: 0.4309 - acc: 0.6238 - f1_m: 0.6238 - precision_m: 0.6238 - recall_m: 0.6238 - val_loss: 0.4927 - val_acc: 0.5757 - val_f1_m: 0.5757 - val_precision_m: 0.5757 - val_recall_m: 0.5757\n",
      "Epoch 81/200\n",
      "45300/45300 [==============================] - 31s 680us/step - loss: 0.4331 - acc: 0.6201 - f1_m: 0.6201 - precision_m: 0.6201 - recall_m: 0.6201 - val_loss: 0.5269 - val_acc: 0.5161 - val_f1_m: 0.5161 - val_precision_m: 0.5161 - val_recall_m: 0.5161\n",
      "Epoch 82/200\n",
      "45300/45300 [==============================] - 31s 683us/step - loss: 0.4350 - acc: 0.6200 - f1_m: 0.6200 - precision_m: 0.6200 - recall_m: 0.6200 - val_loss: 0.5262 - val_acc: 0.4969 - val_f1_m: 0.4969 - val_precision_m: 0.4969 - val_recall_m: 0.4969\n",
      "Epoch 83/200\n",
      "45300/45300 [==============================] - 31s 688us/step - loss: 0.4298 - acc: 0.6202 - f1_m: 0.6202 - precision_m: 0.6202 - recall_m: 0.6202 - val_loss: 0.5131 - val_acc: 0.5179 - val_f1_m: 0.5179 - val_precision_m: 0.5179 - val_recall_m: 0.5179\n",
      "Epoch 84/200\n",
      "45300/45300 [==============================] - 31s 675us/step - loss: 0.4219 - acc: 0.6336 - f1_m: 0.6336 - precision_m: 0.6336 - recall_m: 0.6336 - val_loss: 0.5778 - val_acc: 0.5534 - val_f1_m: 0.5534 - val_precision_m: 0.5534 - val_recall_m: 0.5534\n",
      "Epoch 85/200\n",
      "45300/45300 [==============================] - 30s 667us/step - loss: 0.4354 - acc: 0.6138 - f1_m: 0.6138 - precision_m: 0.6138 - recall_m: 0.6138 - val_loss: 0.4840 - val_acc: 0.5709 - val_f1_m: 0.5709 - val_precision_m: 0.5709 - val_recall_m: 0.5709\n",
      "Epoch 86/200\n",
      "45300/45300 [==============================] - 31s 681us/step - loss: 0.4270 - acc: 0.6218 - f1_m: 0.6218 - precision_m: 0.6218 - recall_m: 0.6218 - val_loss: 0.4872 - val_acc: 0.5530 - val_f1_m: 0.5530 - val_precision_m: 0.5530 - val_recall_m: 0.5530\n",
      "Epoch 87/200\n",
      "45300/45300 [==============================] - 32s 696us/step - loss: 0.4391 - acc: 0.6168 - f1_m: 0.6168 - precision_m: 0.6168 - recall_m: 0.6168 - val_loss: 0.5431 - val_acc: 0.5810 - val_f1_m: 0.5810 - val_precision_m: 0.5810 - val_recall_m: 0.5810\n",
      "Epoch 88/200\n",
      "45300/45300 [==============================] - 31s 677us/step - loss: 0.4339 - acc: 0.6181 - f1_m: 0.6181 - precision_m: 0.6181 - recall_m: 0.6181 - val_loss: 0.4813 - val_acc: 0.5711 - val_f1_m: 0.5711 - val_precision_m: 0.5711 - val_recall_m: 0.5711\n",
      "Epoch 89/200\n",
      "45300/45300 [==============================] - 31s 685us/step - loss: 0.4182 - acc: 0.6340 - f1_m: 0.6340 - precision_m: 0.6340 - recall_m: 0.6340 - val_loss: 0.4819 - val_acc: 0.5627 - val_f1_m: 0.5627 - val_precision_m: 0.5627 - val_recall_m: 0.5627\n",
      "Epoch 90/200\n",
      "45300/45300 [==============================] - 31s 676us/step - loss: 0.4263 - acc: 0.6260 - f1_m: 0.6260 - precision_m: 0.6260 - recall_m: 0.6260 - val_loss: 0.4980 - val_acc: 0.5640 - val_f1_m: 0.5640 - val_precision_m: 0.5640 - val_recall_m: 0.5640\n",
      "Epoch 91/200\n",
      "45300/45300 [==============================] - 31s 694us/step - loss: 0.4267 - acc: 0.6199 - f1_m: 0.6199 - precision_m: 0.6200 - recall_m: 0.6199 - val_loss: 0.4913 - val_acc: 0.5801 - val_f1_m: 0.5801 - val_precision_m: 0.5801 - val_recall_m: 0.5801\n",
      "Epoch 92/200\n",
      "45300/45300 [==============================] - 32s 705us/step - loss: 0.4183 - acc: 0.6318 - f1_m: 0.6318 - precision_m: 0.6318 - recall_m: 0.6318 - val_loss: 0.5239 - val_acc: 0.5870 - val_f1_m: 0.5870 - val_precision_m: 0.5870 - val_recall_m: 0.5870\n",
      "Epoch 93/200\n",
      "45300/45300 [==============================] - 31s 683us/step - loss: 0.4244 - acc: 0.6217 - f1_m: 0.6217 - precision_m: 0.6217 - recall_m: 0.6217 - val_loss: 0.5384 - val_acc: 0.4879 - val_f1_m: 0.4877 - val_precision_m: 0.4877 - val_recall_m: 0.4876\n",
      "Epoch 94/200\n",
      "45300/45300 [==============================] - 31s 680us/step - loss: 0.4140 - acc: 0.6340 - f1_m: 0.6340 - precision_m: 0.6341 - recall_m: 0.6340 - val_loss: 0.4793 - val_acc: 0.5810 - val_f1_m: 0.5810 - val_precision_m: 0.5810 - val_recall_m: 0.5810\n",
      "Epoch 95/200\n",
      "45300/45300 [==============================] - 31s 688us/step - loss: 0.4141 - acc: 0.6334 - f1_m: 0.6334 - precision_m: 0.6334 - recall_m: 0.6334 - val_loss: 0.4981 - val_acc: 0.5918 - val_f1_m: 0.5918 - val_precision_m: 0.5918 - val_recall_m: 0.5918\n",
      "Epoch 96/200\n",
      "45300/45300 [==============================] - 32s 696us/step - loss: 0.4250 - acc: 0.6273 - f1_m: 0.6273 - precision_m: 0.6273 - recall_m: 0.6273 - val_loss: 0.4872 - val_acc: 0.5717 - val_f1_m: 0.5717 - val_precision_m: 0.5717 - val_recall_m: 0.5717\n",
      "Epoch 97/200\n",
      "45300/45300 [==============================] - 31s 693us/step - loss: 0.4303 - acc: 0.6271 - f1_m: 0.6271 - precision_m: 0.6271 - recall_m: 0.6271 - val_loss: 0.5229 - val_acc: 0.5799 - val_f1_m: 0.5799 - val_precision_m: 0.5799 - val_recall_m: 0.5799\n",
      "Epoch 98/200\n",
      "45300/45300 [==============================] - 30s 671us/step - loss: 0.4218 - acc: 0.6338 - f1_m: 0.6338 - precision_m: 0.6338 - recall_m: 0.6338 - val_loss: 0.5572 - val_acc: 0.4735 - val_f1_m: 0.4735 - val_precision_m: 0.4735 - val_recall_m: 0.4735\n",
      "Epoch 99/200\n",
      "45300/45300 [==============================] - 31s 675us/step - loss: 0.4326 - acc: 0.6180 - f1_m: 0.6180 - precision_m: 0.6180 - recall_m: 0.6180 - val_loss: 0.4881 - val_acc: 0.5667 - val_f1_m: 0.5667 - val_precision_m: 0.5667 - val_recall_m: 0.5667\n",
      "Epoch 100/200\n",
      "45300/45300 [==============================] - 31s 683us/step - loss: 0.4156 - acc: 0.6394 - f1_m: 0.6394 - precision_m: 0.6394 - recall_m: 0.6394 - val_loss: 0.5314 - val_acc: 0.5095 - val_f1_m: 0.5095 - val_precision_m: 0.5095 - val_recall_m: 0.5095\n",
      "Epoch 101/200\n",
      " 4032/45300 [=>............................] - ETA: 27s - loss: 0.4418 - acc: 0.6074 - f1_m: 0.6074 - precision_m: 0.6074 - recall_m: 0.6074Buffered data was truncated after reaching the output size limit."
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "mdl = w2vmodel(embedding_size=200, max_words=MAX_SEQUENCE_LENGTH, vocabulary_size=len(vocabulary)+1,\n",
    "            y_dim=y_train.shape[1],filter_sizes = [3,4,5],dropout=0.1)\n",
    "mdl.compile(loss=f1_loss,#'categorical_crossentropy', \n",
    "            optimizer='adam', \n",
    "            metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 200\n",
    "\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint('w2v_saved_model.h5', verbose=0, monitor='val_loss',save_best_only=True, mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=0, epsilon=1e-4, mode='min')\n",
    "\n",
    "\n",
    "history = mdl.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=num_epochs, \n",
    "          callbacks=[\n",
    "                     #earlyStopping, \n",
    "                     mcp_save]\n",
    "          )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "esOm5UXBLqVS"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(history.epoch,history.history['loss'],history.epoch,history.history['val_loss'], linewidth=2)\n",
    "# plt.plot(history.epoch,history.history['f1_m'],history.epoch,history.history['val_f1_m'])\n",
    "plt.ylabel('Loss', fontsize=15)\n",
    "plt.xlabel('Epoch', fontsize=15)\n",
    "plt.legend(['Training loss', 'Validation loss'], fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "ROND0HQGLwTz",
    "outputId": "4acbec76-08ed-45a3-a2d4-b15117202bc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 1s 2ms/step\n",
      "loss: 0.4751, accuracy: 0.5893, f1-score: 0.5893, precision: 0.5893, recall: 0.5893\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "mdl = load_model('w2v_saved_model.h5', custom_objects={\n",
    "                                                  'f1_loss':f1_loss, \n",
    "                                                  'f1_m':f1_m,\n",
    "                                                  'precision_m':precision_m,\n",
    "                                                  'recall_m':recall_m\n",
    "                                                   })\n",
    "loss, acc, f1, prec, rec = mdl.evaluate(X_test, y_test)\n",
    "print(\"loss: {}, accuracy: {}, f1-score: {}, precision: {}, recall: {}\".format(round(loss,4), round(acc,4), round(f1,4), round(prec,4), round(rec,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "btA-2yMu9-UY",
    "outputId": "e23998d5-1575-464f-887a-fc62cf08fcdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_6/embedding_lookup/Identity:0\", shape=(?, 59, 100), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "\n",
    "mdl = model(embedding_size=100, max_words=MAX_SEQUENCE_LENGTH, vocabulary_size=len(vocabulary)+1,\n",
    "            y_dim=y_train.shape[1],filter_sizes = [3,4,5],dropout=0.1)\n",
    "mdl.compile(loss=f1_loss,#'categorical_crossentropy', \n",
    "            optimizer='adam', \n",
    "            metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "pyi-jMJ7OeEB",
    "outputId": "0525cae2-ed50-4d9a-db65-832dcf36e6f8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1335: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45300 samples, validate on 4530 samples\n",
      "Epoch 1/200\n",
      "45300/45300 [==============================] - 27s 586us/step - loss: 0.5015 - acc: 0.5608 - f1_m: 0.5300 - precision_m: 0.5585 - recall_m: 0.5210 - val_loss: 0.4473 - val_acc: 0.5954 - val_f1_m: 0.5954 - val_precision_m: 0.5954 - val_recall_m: 0.5954\n",
      "Epoch 2/200\n",
      "45300/45300 [==============================] - 22s 494us/step - loss: 0.4171 - acc: 0.6278 - f1_m: 0.6278 - precision_m: 0.6280 - recall_m: 0.6275 - val_loss: 0.4410 - val_acc: 0.6084 - val_f1_m: 0.6084 - val_precision_m: 0.6084 - val_recall_m: 0.6084\n",
      "Epoch 3/200\n",
      "45300/45300 [==============================] - 22s 491us/step - loss: 0.3979 - acc: 0.6440 - f1_m: 0.6440 - precision_m: 0.6442 - recall_m: 0.6438 - val_loss: 0.4416 - val_acc: 0.6046 - val_f1_m: 0.6046 - val_precision_m: 0.6046 - val_recall_m: 0.6046\n",
      "Epoch 4/200\n",
      "45300/45300 [==============================] - 22s 485us/step - loss: 0.3895 - acc: 0.6513 - f1_m: 0.6514 - precision_m: 0.6515 - recall_m: 0.6513 - val_loss: 0.4397 - val_acc: 0.6049 - val_f1_m: 0.6049 - val_precision_m: 0.6049 - val_recall_m: 0.6049\n",
      "Epoch 5/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.3821 - acc: 0.6553 - f1_m: 0.6553 - precision_m: 0.6554 - recall_m: 0.6552 - val_loss: 0.4332 - val_acc: 0.6049 - val_f1_m: 0.6049 - val_precision_m: 0.6049 - val_recall_m: 0.6049\n",
      "Epoch 6/200\n",
      "45300/45300 [==============================] - 22s 488us/step - loss: 0.3752 - acc: 0.6630 - f1_m: 0.6630 - precision_m: 0.6630 - recall_m: 0.6629 - val_loss: 0.4197 - val_acc: 0.6216 - val_f1_m: 0.6216 - val_precision_m: 0.6216 - val_recall_m: 0.6216\n",
      "Epoch 7/200\n",
      "45300/45300 [==============================] - 22s 491us/step - loss: 0.3643 - acc: 0.6772 - f1_m: 0.6772 - precision_m: 0.6772 - recall_m: 0.6771 - val_loss: 0.4154 - val_acc: 0.6280 - val_f1_m: 0.6280 - val_precision_m: 0.6280 - val_recall_m: 0.6280\n",
      "Epoch 8/200\n",
      "45300/45300 [==============================] - 22s 491us/step - loss: 0.3670 - acc: 0.6746 - f1_m: 0.6746 - precision_m: 0.6746 - recall_m: 0.6746 - val_loss: 0.4272 - val_acc: 0.6309 - val_f1_m: 0.6309 - val_precision_m: 0.6309 - val_recall_m: 0.6309\n",
      "Epoch 9/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.3625 - acc: 0.6783 - f1_m: 0.6783 - precision_m: 0.6784 - recall_m: 0.6783 - val_loss: 0.4289 - val_acc: 0.6185 - val_f1_m: 0.6185 - val_precision_m: 0.6185 - val_recall_m: 0.6185\n",
      "Epoch 10/200\n",
      "45300/45300 [==============================] - 22s 483us/step - loss: 0.3574 - acc: 0.6802 - f1_m: 0.6802 - precision_m: 0.6802 - recall_m: 0.6801 - val_loss: 0.4164 - val_acc: 0.6300 - val_f1_m: 0.6300 - val_precision_m: 0.6300 - val_recall_m: 0.6300\n",
      "Epoch 11/200\n",
      "45300/45300 [==============================] - 22s 485us/step - loss: 0.3540 - acc: 0.6838 - f1_m: 0.6838 - precision_m: 0.6838 - recall_m: 0.6838 - val_loss: 0.4278 - val_acc: 0.6157 - val_f1_m: 0.6157 - val_precision_m: 0.6157 - val_recall_m: 0.6157\n",
      "Epoch 12/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.3498 - acc: 0.6878 - f1_m: 0.6878 - precision_m: 0.6878 - recall_m: 0.6878 - val_loss: 0.4277 - val_acc: 0.6302 - val_f1_m: 0.6302 - val_precision_m: 0.6302 - val_recall_m: 0.6302\n",
      "Epoch 13/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.3457 - acc: 0.6915 - f1_m: 0.6915 - precision_m: 0.6915 - recall_m: 0.6915 - val_loss: 0.4212 - val_acc: 0.6325 - val_f1_m: 0.6325 - val_precision_m: 0.6325 - val_recall_m: 0.6325\n",
      "Epoch 14/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.3397 - acc: 0.6968 - f1_m: 0.6968 - precision_m: 0.6968 - recall_m: 0.6968 - val_loss: 0.4195 - val_acc: 0.6245 - val_f1_m: 0.6245 - val_precision_m: 0.6245 - val_recall_m: 0.6245\n",
      "Epoch 15/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.3371 - acc: 0.7015 - f1_m: 0.7015 - precision_m: 0.7015 - recall_m: 0.7015 - val_loss: 0.4299 - val_acc: 0.6230 - val_f1_m: 0.6230 - val_precision_m: 0.6230 - val_recall_m: 0.6230\n",
      "Epoch 16/200\n",
      "45300/45300 [==============================] - 22s 482us/step - loss: 0.3303 - acc: 0.7066 - f1_m: 0.7066 - precision_m: 0.7067 - recall_m: 0.7066 - val_loss: 0.4317 - val_acc: 0.6177 - val_f1_m: 0.6177 - val_precision_m: 0.6177 - val_recall_m: 0.6177\n",
      "Epoch 17/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.3271 - acc: 0.7096 - f1_m: 0.7096 - precision_m: 0.7096 - recall_m: 0.7096 - val_loss: 0.4202 - val_acc: 0.6305 - val_f1_m: 0.6305 - val_precision_m: 0.6305 - val_recall_m: 0.6305\n",
      "Epoch 18/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.3305 - acc: 0.7066 - f1_m: 0.7066 - precision_m: 0.7066 - recall_m: 0.7066 - val_loss: 0.4381 - val_acc: 0.6141 - val_f1_m: 0.6141 - val_precision_m: 0.6141 - val_recall_m: 0.6141\n",
      "Epoch 19/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.3258 - acc: 0.7123 - f1_m: 0.7123 - precision_m: 0.7123 - recall_m: 0.7123 - val_loss: 0.4299 - val_acc: 0.6258 - val_f1_m: 0.6258 - val_precision_m: 0.6258 - val_recall_m: 0.6258\n",
      "Epoch 20/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.3241 - acc: 0.7138 - f1_m: 0.7138 - precision_m: 0.7138 - recall_m: 0.7137 - val_loss: 0.4264 - val_acc: 0.6260 - val_f1_m: 0.6260 - val_precision_m: 0.6260 - val_recall_m: 0.6260\n",
      "Epoch 21/200\n",
      "45300/45300 [==============================] - 22s 487us/step - loss: 0.3202 - acc: 0.7162 - f1_m: 0.7162 - precision_m: 0.7162 - recall_m: 0.7162 - val_loss: 0.4308 - val_acc: 0.6267 - val_f1_m: 0.6267 - val_precision_m: 0.6267 - val_recall_m: 0.6267\n",
      "Epoch 22/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.3180 - acc: 0.7183 - f1_m: 0.7183 - precision_m: 0.7183 - recall_m: 0.7183 - val_loss: 0.4295 - val_acc: 0.6254 - val_f1_m: 0.6254 - val_precision_m: 0.6254 - val_recall_m: 0.6254\n",
      "Epoch 23/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.3184 - acc: 0.7194 - f1_m: 0.7194 - precision_m: 0.7194 - recall_m: 0.7194 - val_loss: 0.4250 - val_acc: 0.6360 - val_f1_m: 0.6360 - val_precision_m: 0.6360 - val_recall_m: 0.6360\n",
      "Epoch 24/200\n",
      "45300/45300 [==============================] - 22s 484us/step - loss: 0.3149 - acc: 0.7225 - f1_m: 0.7225 - precision_m: 0.7225 - recall_m: 0.7225 - val_loss: 0.4242 - val_acc: 0.6351 - val_f1_m: 0.6351 - val_precision_m: 0.6351 - val_recall_m: 0.6351\n",
      "Epoch 25/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.3166 - acc: 0.7202 - f1_m: 0.7202 - precision_m: 0.7202 - recall_m: 0.7202 - val_loss: 0.4332 - val_acc: 0.6322 - val_f1_m: 0.6322 - val_precision_m: 0.6322 - val_recall_m: 0.6322\n",
      "Epoch 26/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.3135 - acc: 0.7256 - f1_m: 0.7256 - precision_m: 0.7256 - recall_m: 0.7256 - val_loss: 0.4452 - val_acc: 0.6311 - val_f1_m: 0.6311 - val_precision_m: 0.6311 - val_recall_m: 0.6311\n",
      "Epoch 27/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.3091 - acc: 0.7287 - f1_m: 0.7287 - precision_m: 0.7287 - recall_m: 0.7287 - val_loss: 0.4272 - val_acc: 0.6400 - val_f1_m: 0.6400 - val_precision_m: 0.6400 - val_recall_m: 0.6400\n",
      "Epoch 28/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.3050 - acc: 0.7307 - f1_m: 0.7307 - precision_m: 0.7307 - recall_m: 0.7307 - val_loss: 0.4257 - val_acc: 0.6254 - val_f1_m: 0.6254 - val_precision_m: 0.6254 - val_recall_m: 0.6254\n",
      "Epoch 29/200\n",
      "45300/45300 [==============================] - 22s 476us/step - loss: 0.3020 - acc: 0.7336 - f1_m: 0.7336 - precision_m: 0.7336 - recall_m: 0.7336 - val_loss: 0.4215 - val_acc: 0.6364 - val_f1_m: 0.6364 - val_precision_m: 0.6364 - val_recall_m: 0.6364\n",
      "Epoch 30/200\n",
      "45300/45300 [==============================] - 22s 482us/step - loss: 0.3023 - acc: 0.7343 - f1_m: 0.7343 - precision_m: 0.7344 - recall_m: 0.7343 - val_loss: 0.4183 - val_acc: 0.6426 - val_f1_m: 0.6426 - val_precision_m: 0.6426 - val_recall_m: 0.6426\n",
      "Epoch 31/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.2971 - acc: 0.7354 - f1_m: 0.7354 - precision_m: 0.7354 - recall_m: 0.7354 - val_loss: 0.4229 - val_acc: 0.6389 - val_f1_m: 0.6389 - val_precision_m: 0.6389 - val_recall_m: 0.6389\n",
      "Epoch 32/200\n",
      "45300/45300 [==============================] - 22s 482us/step - loss: 0.2920 - acc: 0.7417 - f1_m: 0.7417 - precision_m: 0.7417 - recall_m: 0.7417 - val_loss: 0.4290 - val_acc: 0.6298 - val_f1_m: 0.6298 - val_precision_m: 0.6298 - val_recall_m: 0.6298\n",
      "Epoch 33/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2899 - acc: 0.7430 - f1_m: 0.7430 - precision_m: 0.7430 - recall_m: 0.7430 - val_loss: 0.4256 - val_acc: 0.6351 - val_f1_m: 0.6351 - val_precision_m: 0.6351 - val_recall_m: 0.6351\n",
      "Epoch 34/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2900 - acc: 0.7436 - f1_m: 0.7436 - precision_m: 0.7436 - recall_m: 0.7436 - val_loss: 0.4218 - val_acc: 0.6366 - val_f1_m: 0.6366 - val_precision_m: 0.6366 - val_recall_m: 0.6366\n",
      "Epoch 35/200\n",
      "45300/45300 [==============================] - 22s 485us/step - loss: 0.2878 - acc: 0.7449 - f1_m: 0.7449 - precision_m: 0.7449 - recall_m: 0.7449 - val_loss: 0.4307 - val_acc: 0.6355 - val_f1_m: 0.6355 - val_precision_m: 0.6355 - val_recall_m: 0.6355\n",
      "Epoch 36/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.2878 - acc: 0.7430 - f1_m: 0.7430 - precision_m: 0.7430 - recall_m: 0.7430 - val_loss: 0.4138 - val_acc: 0.6296 - val_f1_m: 0.6296 - val_precision_m: 0.6296 - val_recall_m: 0.6296\n",
      "Epoch 37/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.2896 - acc: 0.7433 - f1_m: 0.7433 - precision_m: 0.7433 - recall_m: 0.7433 - val_loss: 0.4240 - val_acc: 0.6294 - val_f1_m: 0.6294 - val_precision_m: 0.6294 - val_recall_m: 0.6294\n",
      "Epoch 38/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2845 - acc: 0.7459 - f1_m: 0.7459 - precision_m: 0.7459 - recall_m: 0.7459 - val_loss: 0.4177 - val_acc: 0.6358 - val_f1_m: 0.6358 - val_precision_m: 0.6358 - val_recall_m: 0.6358\n",
      "Epoch 39/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2828 - acc: 0.7498 - f1_m: 0.7498 - precision_m: 0.7498 - recall_m: 0.7498 - val_loss: 0.4164 - val_acc: 0.6411 - val_f1_m: 0.6411 - val_precision_m: 0.6411 - val_recall_m: 0.6411\n",
      "Epoch 40/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2815 - acc: 0.7527 - f1_m: 0.7527 - precision_m: 0.7527 - recall_m: 0.7527 - val_loss: 0.4165 - val_acc: 0.6382 - val_f1_m: 0.6382 - val_precision_m: 0.6382 - val_recall_m: 0.6382\n",
      "Epoch 41/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2782 - acc: 0.7526 - f1_m: 0.7526 - precision_m: 0.7526 - recall_m: 0.7526 - val_loss: 0.4176 - val_acc: 0.6327 - val_f1_m: 0.6327 - val_precision_m: 0.6327 - val_recall_m: 0.6327\n",
      "Epoch 42/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2777 - acc: 0.7516 - f1_m: 0.7516 - precision_m: 0.7516 - recall_m: 0.7516 - val_loss: 0.4239 - val_acc: 0.6236 - val_f1_m: 0.6236 - val_precision_m: 0.6236 - val_recall_m: 0.6236\n",
      "Epoch 43/200\n",
      "45300/45300 [==============================] - 22s 483us/step - loss: 0.2773 - acc: 0.7527 - f1_m: 0.7527 - precision_m: 0.7527 - recall_m: 0.7527 - val_loss: 0.4242 - val_acc: 0.6230 - val_f1_m: 0.6230 - val_precision_m: 0.6230 - val_recall_m: 0.6230\n",
      "Epoch 44/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.2786 - acc: 0.7532 - f1_m: 0.7532 - precision_m: 0.7532 - recall_m: 0.7532 - val_loss: 0.4239 - val_acc: 0.6320 - val_f1_m: 0.6320 - val_precision_m: 0.6320 - val_recall_m: 0.6320\n",
      "Epoch 45/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2761 - acc: 0.7565 - f1_m: 0.7566 - precision_m: 0.7566 - recall_m: 0.7565 - val_loss: 0.4201 - val_acc: 0.6300 - val_f1_m: 0.6300 - val_precision_m: 0.6300 - val_recall_m: 0.6300\n",
      "Epoch 46/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.2727 - acc: 0.7567 - f1_m: 0.7567 - precision_m: 0.7567 - recall_m: 0.7567 - val_loss: 0.4284 - val_acc: 0.6201 - val_f1_m: 0.6201 - val_precision_m: 0.6201 - val_recall_m: 0.6201\n",
      "Epoch 47/200\n",
      "45300/45300 [==============================] - 22s 482us/step - loss: 0.2726 - acc: 0.7591 - f1_m: 0.7591 - precision_m: 0.7591 - recall_m: 0.7591 - val_loss: 0.4236 - val_acc: 0.6291 - val_f1_m: 0.6291 - val_precision_m: 0.6291 - val_recall_m: 0.6291\n",
      "Epoch 48/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2692 - acc: 0.7629 - f1_m: 0.7629 - precision_m: 0.7629 - recall_m: 0.7629 - val_loss: 0.4230 - val_acc: 0.6322 - val_f1_m: 0.6322 - val_precision_m: 0.6322 - val_recall_m: 0.6322\n",
      "Epoch 49/200\n",
      "45300/45300 [==============================] - 22s 485us/step - loss: 0.2699 - acc: 0.7605 - f1_m: 0.7605 - precision_m: 0.7605 - recall_m: 0.7605 - val_loss: 0.4234 - val_acc: 0.6375 - val_f1_m: 0.6375 - val_precision_m: 0.6375 - val_recall_m: 0.6375\n",
      "Epoch 50/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2704 - acc: 0.7602 - f1_m: 0.7602 - precision_m: 0.7602 - recall_m: 0.7602 - val_loss: 0.4235 - val_acc: 0.6320 - val_f1_m: 0.6320 - val_precision_m: 0.6320 - val_recall_m: 0.6320\n",
      "Epoch 51/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2759 - acc: 0.7563 - f1_m: 0.7563 - precision_m: 0.7564 - recall_m: 0.7563 - val_loss: 0.4254 - val_acc: 0.6389 - val_f1_m: 0.6389 - val_precision_m: 0.6389 - val_recall_m: 0.6389\n",
      "Epoch 52/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2754 - acc: 0.7560 - f1_m: 0.7560 - precision_m: 0.7560 - recall_m: 0.7560 - val_loss: 0.4294 - val_acc: 0.6236 - val_f1_m: 0.6236 - val_precision_m: 0.6236 - val_recall_m: 0.6236\n",
      "Epoch 53/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2650 - acc: 0.7658 - f1_m: 0.7658 - precision_m: 0.7658 - recall_m: 0.7658 - val_loss: 0.4254 - val_acc: 0.6355 - val_f1_m: 0.6355 - val_precision_m: 0.6355 - val_recall_m: 0.6355\n",
      "Epoch 54/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.2656 - acc: 0.7649 - f1_m: 0.7649 - precision_m: 0.7650 - recall_m: 0.7649 - val_loss: 0.4178 - val_acc: 0.6382 - val_f1_m: 0.6382 - val_precision_m: 0.6382 - val_recall_m: 0.6382\n",
      "Epoch 55/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2637 - acc: 0.7680 - f1_m: 0.7681 - precision_m: 0.7681 - recall_m: 0.7680 - val_loss: 0.4178 - val_acc: 0.6406 - val_f1_m: 0.6406 - val_precision_m: 0.6406 - val_recall_m: 0.6406\n",
      "Epoch 56/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2620 - acc: 0.7708 - f1_m: 0.7708 - precision_m: 0.7708 - recall_m: 0.7708 - val_loss: 0.4136 - val_acc: 0.6406 - val_f1_m: 0.6406 - val_precision_m: 0.6406 - val_recall_m: 0.6406\n",
      "Epoch 57/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2575 - acc: 0.7736 - f1_m: 0.7736 - precision_m: 0.7736 - recall_m: 0.7736 - val_loss: 0.4167 - val_acc: 0.6419 - val_f1_m: 0.6419 - val_precision_m: 0.6419 - val_recall_m: 0.6419\n",
      "Epoch 58/200\n",
      "45300/45300 [==============================] - 22s 483us/step - loss: 0.2592 - acc: 0.7721 - f1_m: 0.7721 - precision_m: 0.7721 - recall_m: 0.7721 - val_loss: 0.4274 - val_acc: 0.6406 - val_f1_m: 0.6406 - val_precision_m: 0.6406 - val_recall_m: 0.6406\n",
      "Epoch 59/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2594 - acc: 0.7710 - f1_m: 0.7710 - precision_m: 0.7710 - recall_m: 0.7710 - val_loss: 0.4294 - val_acc: 0.6256 - val_f1_m: 0.6256 - val_precision_m: 0.6256 - val_recall_m: 0.6256\n",
      "Epoch 60/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.2556 - acc: 0.7745 - f1_m: 0.7745 - precision_m: 0.7745 - recall_m: 0.7745 - val_loss: 0.4244 - val_acc: 0.6366 - val_f1_m: 0.6366 - val_precision_m: 0.6366 - val_recall_m: 0.6366\n",
      "Epoch 61/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2530 - acc: 0.7770 - f1_m: 0.7770 - precision_m: 0.7770 - recall_m: 0.7770 - val_loss: 0.4220 - val_acc: 0.6316 - val_f1_m: 0.6316 - val_precision_m: 0.6316 - val_recall_m: 0.6316\n",
      "Epoch 62/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2503 - acc: 0.7782 - f1_m: 0.7782 - precision_m: 0.7782 - recall_m: 0.7782 - val_loss: 0.4230 - val_acc: 0.6307 - val_f1_m: 0.6307 - val_precision_m: 0.6307 - val_recall_m: 0.6307\n",
      "Epoch 63/200\n",
      "45300/45300 [==============================] - 22s 482us/step - loss: 0.2532 - acc: 0.7766 - f1_m: 0.7766 - precision_m: 0.7766 - recall_m: 0.7766 - val_loss: 0.4205 - val_acc: 0.6287 - val_f1_m: 0.6287 - val_precision_m: 0.6287 - val_recall_m: 0.6287\n",
      "Epoch 64/200\n",
      "45300/45300 [==============================] - 22s 483us/step - loss: 0.2627 - acc: 0.7700 - f1_m: 0.7700 - precision_m: 0.7700 - recall_m: 0.7700 - val_loss: 0.4248 - val_acc: 0.6347 - val_f1_m: 0.6347 - val_precision_m: 0.6347 - val_recall_m: 0.6347\n",
      "Epoch 65/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.2550 - acc: 0.7760 - f1_m: 0.7759 - precision_m: 0.7760 - recall_m: 0.7759 - val_loss: 0.4184 - val_acc: 0.6320 - val_f1_m: 0.6320 - val_precision_m: 0.6320 - val_recall_m: 0.6320\n",
      "Epoch 66/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2503 - acc: 0.7775 - f1_m: 0.7775 - precision_m: 0.7775 - recall_m: 0.7775 - val_loss: 0.4251 - val_acc: 0.6349 - val_f1_m: 0.6349 - val_precision_m: 0.6349 - val_recall_m: 0.6349\n",
      "Epoch 67/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2525 - acc: 0.7765 - f1_m: 0.7765 - precision_m: 0.7765 - recall_m: 0.7765 - val_loss: 0.4235 - val_acc: 0.6291 - val_f1_m: 0.6291 - val_precision_m: 0.6291 - val_recall_m: 0.6291\n",
      "Epoch 68/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.2514 - acc: 0.7761 - f1_m: 0.7761 - precision_m: 0.7761 - recall_m: 0.7761 - val_loss: 0.4260 - val_acc: 0.6238 - val_f1_m: 0.6238 - val_precision_m: 0.6238 - val_recall_m: 0.6238\n",
      "Epoch 69/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.2520 - acc: 0.7766 - f1_m: 0.7766 - precision_m: 0.7766 - recall_m: 0.7766 - val_loss: 0.4238 - val_acc: 0.6249 - val_f1_m: 0.6249 - val_precision_m: 0.6249 - val_recall_m: 0.6249\n",
      "Epoch 70/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2478 - acc: 0.7794 - f1_m: 0.7794 - precision_m: 0.7794 - recall_m: 0.7794 - val_loss: 0.4226 - val_acc: 0.6322 - val_f1_m: 0.6322 - val_precision_m: 0.6322 - val_recall_m: 0.6322\n",
      "Epoch 71/200\n",
      "45300/45300 [==============================] - 22s 476us/step - loss: 0.2494 - acc: 0.7804 - f1_m: 0.7804 - precision_m: 0.7804 - recall_m: 0.7804 - val_loss: 0.4151 - val_acc: 0.6305 - val_f1_m: 0.6305 - val_precision_m: 0.6305 - val_recall_m: 0.6305\n",
      "Epoch 72/200\n",
      "45300/45300 [==============================] - 22s 482us/step - loss: 0.2450 - acc: 0.7834 - f1_m: 0.7834 - precision_m: 0.7834 - recall_m: 0.7834 - val_loss: 0.4218 - val_acc: 0.6260 - val_f1_m: 0.6260 - val_precision_m: 0.6260 - val_recall_m: 0.6260\n",
      "Epoch 73/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2499 - acc: 0.7810 - f1_m: 0.7810 - precision_m: 0.7810 - recall_m: 0.7810 - val_loss: 0.4190 - val_acc: 0.6322 - val_f1_m: 0.6322 - val_precision_m: 0.6322 - val_recall_m: 0.6322\n",
      "Epoch 74/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2489 - acc: 0.7823 - f1_m: 0.7823 - precision_m: 0.7823 - recall_m: 0.7823 - val_loss: 0.4181 - val_acc: 0.6318 - val_f1_m: 0.6318 - val_precision_m: 0.6318 - val_recall_m: 0.6318\n",
      "Epoch 75/200\n",
      "45300/45300 [==============================] - 22s 477us/step - loss: 0.2505 - acc: 0.7798 - f1_m: 0.7798 - precision_m: 0.7798 - recall_m: 0.7798 - val_loss: 0.4243 - val_acc: 0.6322 - val_f1_m: 0.6322 - val_precision_m: 0.6322 - val_recall_m: 0.6322\n",
      "Epoch 76/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.2475 - acc: 0.7819 - f1_m: 0.7819 - precision_m: 0.7819 - recall_m: 0.7819 - val_loss: 0.4264 - val_acc: 0.6395 - val_f1_m: 0.6395 - val_precision_m: 0.6395 - val_recall_m: 0.6395\n",
      "Epoch 77/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.2482 - acc: 0.7814 - f1_m: 0.7814 - precision_m: 0.7815 - recall_m: 0.7814 - val_loss: 0.4193 - val_acc: 0.6307 - val_f1_m: 0.6307 - val_precision_m: 0.6307 - val_recall_m: 0.6307\n",
      "Epoch 78/200\n",
      "45300/45300 [==============================] - 22s 487us/step - loss: 0.2564 - acc: 0.7766 - f1_m: 0.7766 - precision_m: 0.7766 - recall_m: 0.7766 - val_loss: 0.4289 - val_acc: 0.6331 - val_f1_m: 0.6331 - val_precision_m: 0.6331 - val_recall_m: 0.6331\n",
      "Epoch 79/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.2509 - acc: 0.7810 - f1_m: 0.7810 - precision_m: 0.7810 - recall_m: 0.7810 - val_loss: 0.4252 - val_acc: 0.6311 - val_f1_m: 0.6311 - val_precision_m: 0.6311 - val_recall_m: 0.6311\n",
      "Epoch 80/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.2568 - acc: 0.7789 - f1_m: 0.7789 - precision_m: 0.7789 - recall_m: 0.7789 - val_loss: 0.4297 - val_acc: 0.6316 - val_f1_m: 0.6316 - val_precision_m: 0.6316 - val_recall_m: 0.6316\n",
      "Epoch 81/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2499 - acc: 0.7800 - f1_m: 0.7801 - precision_m: 0.7801 - recall_m: 0.7800 - val_loss: 0.4287 - val_acc: 0.6247 - val_f1_m: 0.6247 - val_precision_m: 0.6247 - val_recall_m: 0.6247\n",
      "Epoch 82/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.2499 - acc: 0.7805 - f1_m: 0.7805 - precision_m: 0.7805 - recall_m: 0.7804 - val_loss: 0.4147 - val_acc: 0.6386 - val_f1_m: 0.6386 - val_precision_m: 0.6386 - val_recall_m: 0.6386\n",
      "Epoch 83/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2426 - acc: 0.7871 - f1_m: 0.7871 - precision_m: 0.7871 - recall_m: 0.7870 - val_loss: 0.4136 - val_acc: 0.6382 - val_f1_m: 0.6382 - val_precision_m: 0.6382 - val_recall_m: 0.6382\n",
      "Epoch 84/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.2487 - acc: 0.7830 - f1_m: 0.7830 - precision_m: 0.7830 - recall_m: 0.7830 - val_loss: 0.4176 - val_acc: 0.6311 - val_f1_m: 0.6310 - val_precision_m: 0.6311 - val_recall_m: 0.6309\n",
      "Epoch 85/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2495 - acc: 0.7797 - f1_m: 0.7797 - precision_m: 0.7797 - recall_m: 0.7796 - val_loss: 0.4249 - val_acc: 0.6278 - val_f1_m: 0.6278 - val_precision_m: 0.6278 - val_recall_m: 0.6278\n",
      "Epoch 86/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.2419 - acc: 0.7868 - f1_m: 0.7868 - precision_m: 0.7868 - recall_m: 0.7868 - val_loss: 0.4172 - val_acc: 0.6289 - val_f1_m: 0.6290 - val_precision_m: 0.6290 - val_recall_m: 0.6289\n",
      "Epoch 87/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.2410 - acc: 0.7872 - f1_m: 0.7872 - precision_m: 0.7872 - recall_m: 0.7872 - val_loss: 0.4149 - val_acc: 0.6369 - val_f1_m: 0.6369 - val_precision_m: 0.6369 - val_recall_m: 0.6369\n",
      "Epoch 88/200\n",
      "45300/45300 [==============================] - 22s 483us/step - loss: 0.2391 - acc: 0.7886 - f1_m: 0.7886 - precision_m: 0.7886 - recall_m: 0.7886 - val_loss: 0.4212 - val_acc: 0.6353 - val_f1_m: 0.6353 - val_precision_m: 0.6353 - val_recall_m: 0.6353\n",
      "Epoch 89/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2396 - acc: 0.7885 - f1_m: 0.7885 - precision_m: 0.7885 - recall_m: 0.7885 - val_loss: 0.4258 - val_acc: 0.6355 - val_f1_m: 0.6355 - val_precision_m: 0.6355 - val_recall_m: 0.6355\n",
      "Epoch 90/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.2437 - acc: 0.7885 - f1_m: 0.7885 - precision_m: 0.7885 - recall_m: 0.7885 - val_loss: 0.4247 - val_acc: 0.6291 - val_f1_m: 0.6291 - val_precision_m: 0.6291 - val_recall_m: 0.6291\n",
      "Epoch 91/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.2378 - acc: 0.7911 - f1_m: 0.7911 - precision_m: 0.7911 - recall_m: 0.7911 - val_loss: 0.4275 - val_acc: 0.6316 - val_f1_m: 0.6316 - val_precision_m: 0.6316 - val_recall_m: 0.6316\n",
      "Epoch 92/200\n",
      "45300/45300 [==============================] - 22s 486us/step - loss: 0.2446 - acc: 0.7855 - f1_m: 0.7855 - precision_m: 0.7855 - recall_m: 0.7855 - val_loss: 0.4209 - val_acc: 0.6428 - val_f1_m: 0.6428 - val_precision_m: 0.6428 - val_recall_m: 0.6428\n",
      "Epoch 93/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.2447 - acc: 0.7839 - f1_m: 0.7839 - precision_m: 0.7839 - recall_m: 0.7839 - val_loss: 0.4152 - val_acc: 0.6393 - val_f1_m: 0.6393 - val_precision_m: 0.6393 - val_recall_m: 0.6393\n",
      "Epoch 94/200\n",
      "45300/45300 [==============================] - 22s 482us/step - loss: 0.2405 - acc: 0.7873 - f1_m: 0.7873 - precision_m: 0.7873 - recall_m: 0.7873 - val_loss: 0.4127 - val_acc: 0.6415 - val_f1_m: 0.6415 - val_precision_m: 0.6415 - val_recall_m: 0.6415\n",
      "Epoch 95/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.2372 - acc: 0.7914 - f1_m: 0.7914 - precision_m: 0.7914 - recall_m: 0.7914 - val_loss: 0.4263 - val_acc: 0.6298 - val_f1_m: 0.6298 - val_precision_m: 0.6298 - val_recall_m: 0.6298\n",
      "Epoch 96/200\n",
      "45300/45300 [==============================] - 22s 477us/step - loss: 0.2427 - acc: 0.7862 - f1_m: 0.7862 - precision_m: 0.7862 - recall_m: 0.7862 - val_loss: 0.4136 - val_acc: 0.6393 - val_f1_m: 0.6393 - val_precision_m: 0.6393 - val_recall_m: 0.6393\n",
      "Epoch 97/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2425 - acc: 0.7902 - f1_m: 0.7902 - precision_m: 0.7902 - recall_m: 0.7902 - val_loss: 0.4335 - val_acc: 0.6433 - val_f1_m: 0.6433 - val_precision_m: 0.6433 - val_recall_m: 0.6433\n",
      "Epoch 98/200\n",
      "45300/45300 [==============================] - 22s 482us/step - loss: 0.2409 - acc: 0.7914 - f1_m: 0.7914 - precision_m: 0.7914 - recall_m: 0.7914 - val_loss: 0.4159 - val_acc: 0.6406 - val_f1_m: 0.6406 - val_precision_m: 0.6406 - val_recall_m: 0.6406\n",
      "Epoch 99/200\n",
      "45300/45300 [==============================] - 22s 483us/step - loss: 0.2401 - acc: 0.7912 - f1_m: 0.7912 - precision_m: 0.7912 - recall_m: 0.7912 - val_loss: 0.4147 - val_acc: 0.6384 - val_f1_m: 0.6384 - val_precision_m: 0.6384 - val_recall_m: 0.6384\n",
      "Epoch 100/200\n",
      "45300/45300 [==============================] - 22s 482us/step - loss: 0.2381 - acc: 0.7917 - f1_m: 0.7917 - precision_m: 0.7917 - recall_m: 0.7917 - val_loss: 0.4070 - val_acc: 0.6382 - val_f1_m: 0.6382 - val_precision_m: 0.6382 - val_recall_m: 0.6382\n",
      "Epoch 101/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2354 - acc: 0.7947 - f1_m: 0.7947 - precision_m: 0.7947 - recall_m: 0.7947 - val_loss: 0.4139 - val_acc: 0.6318 - val_f1_m: 0.6318 - val_precision_m: 0.6318 - val_recall_m: 0.6318\n",
      "Epoch 102/200\n",
      "45300/45300 [==============================] - 22s 483us/step - loss: 0.2336 - acc: 0.7964 - f1_m: 0.7964 - precision_m: 0.7964 - recall_m: 0.7964 - val_loss: 0.4080 - val_acc: 0.6428 - val_f1_m: 0.6428 - val_precision_m: 0.6428 - val_recall_m: 0.6428\n",
      "Epoch 103/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.2353 - acc: 0.7926 - f1_m: 0.7926 - precision_m: 0.7926 - recall_m: 0.7926 - val_loss: 0.4095 - val_acc: 0.6386 - val_f1_m: 0.6386 - val_precision_m: 0.6386 - val_recall_m: 0.6386\n",
      "Epoch 104/200\n",
      "45300/45300 [==============================] - 22s 476us/step - loss: 0.2371 - acc: 0.7909 - f1_m: 0.7909 - precision_m: 0.7909 - recall_m: 0.7909 - val_loss: 0.4145 - val_acc: 0.6349 - val_f1_m: 0.6349 - val_precision_m: 0.6349 - val_recall_m: 0.6349\n",
      "Epoch 105/200\n",
      "45300/45300 [==============================] - 22s 482us/step - loss: 0.2381 - acc: 0.7923 - f1_m: 0.7923 - precision_m: 0.7923 - recall_m: 0.7923 - val_loss: 0.4211 - val_acc: 0.6349 - val_f1_m: 0.6349 - val_precision_m: 0.6349 - val_recall_m: 0.6349\n",
      "Epoch 106/200\n",
      "45300/45300 [==============================] - 22s 489us/step - loss: 0.2346 - acc: 0.7955 - f1_m: 0.7955 - precision_m: 0.7956 - recall_m: 0.7955 - val_loss: 0.4267 - val_acc: 0.6243 - val_f1_m: 0.6243 - val_precision_m: 0.6243 - val_recall_m: 0.6243\n",
      "Epoch 107/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2306 - acc: 0.7973 - f1_m: 0.7973 - precision_m: 0.7973 - recall_m: 0.7973 - val_loss: 0.4282 - val_acc: 0.6296 - val_f1_m: 0.6296 - val_precision_m: 0.6296 - val_recall_m: 0.6296\n",
      "Epoch 108/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2350 - acc: 0.7933 - f1_m: 0.7933 - precision_m: 0.7933 - recall_m: 0.7933 - val_loss: 0.4141 - val_acc: 0.6411 - val_f1_m: 0.6411 - val_precision_m: 0.6411 - val_recall_m: 0.6411\n",
      "Epoch 109/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2351 - acc: 0.7927 - f1_m: 0.7927 - precision_m: 0.7927 - recall_m: 0.7927 - val_loss: 0.4208 - val_acc: 0.6344 - val_f1_m: 0.6344 - val_precision_m: 0.6344 - val_recall_m: 0.6344\n",
      "Epoch 110/200\n",
      "45300/45300 [==============================] - 22s 482us/step - loss: 0.2325 - acc: 0.7951 - f1_m: 0.7951 - precision_m: 0.7951 - recall_m: 0.7951 - val_loss: 0.4141 - val_acc: 0.6285 - val_f1_m: 0.6285 - val_precision_m: 0.6285 - val_recall_m: 0.6285\n",
      "Epoch 111/200\n",
      "45300/45300 [==============================] - 22s 477us/step - loss: 0.2380 - acc: 0.7917 - f1_m: 0.7917 - precision_m: 0.7917 - recall_m: 0.7917 - val_loss: 0.4225 - val_acc: 0.6258 - val_f1_m: 0.6258 - val_precision_m: 0.6258 - val_recall_m: 0.6258\n",
      "Epoch 112/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2357 - acc: 0.7914 - f1_m: 0.7914 - precision_m: 0.7914 - recall_m: 0.7914 - val_loss: 0.4144 - val_acc: 0.6419 - val_f1_m: 0.6419 - val_precision_m: 0.6419 - val_recall_m: 0.6419\n",
      "Epoch 113/200\n",
      "45300/45300 [==============================] - 22s 484us/step - loss: 0.2350 - acc: 0.7931 - f1_m: 0.7931 - precision_m: 0.7931 - recall_m: 0.7931 - val_loss: 0.4164 - val_acc: 0.6342 - val_f1_m: 0.6342 - val_precision_m: 0.6342 - val_recall_m: 0.6342\n",
      "Epoch 114/200\n",
      "45300/45300 [==============================] - 22s 483us/step - loss: 0.2342 - acc: 0.7954 - f1_m: 0.7954 - precision_m: 0.7954 - recall_m: 0.7954 - val_loss: 0.4157 - val_acc: 0.6338 - val_f1_m: 0.6338 - val_precision_m: 0.6338 - val_recall_m: 0.6338\n",
      "Epoch 115/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2310 - acc: 0.7973 - f1_m: 0.7973 - precision_m: 0.7973 - recall_m: 0.7973 - val_loss: 0.4155 - val_acc: 0.6309 - val_f1_m: 0.6309 - val_precision_m: 0.6309 - val_recall_m: 0.6309\n",
      "Epoch 116/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.2326 - acc: 0.7963 - f1_m: 0.7963 - precision_m: 0.7963 - recall_m: 0.7963 - val_loss: 0.4265 - val_acc: 0.6227 - val_f1_m: 0.6227 - val_precision_m: 0.6227 - val_recall_m: 0.6227\n",
      "Epoch 117/200\n",
      "45300/45300 [==============================] - 22s 483us/step - loss: 0.2311 - acc: 0.7966 - f1_m: 0.7966 - precision_m: 0.7966 - recall_m: 0.7966 - val_loss: 0.4230 - val_acc: 0.6291 - val_f1_m: 0.6291 - val_precision_m: 0.6291 - val_recall_m: 0.6291\n",
      "Epoch 118/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2316 - acc: 0.7965 - f1_m: 0.7965 - precision_m: 0.7966 - recall_m: 0.7965 - val_loss: 0.4110 - val_acc: 0.6457 - val_f1_m: 0.6457 - val_precision_m: 0.6457 - val_recall_m: 0.6457\n",
      "Epoch 119/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2317 - acc: 0.7977 - f1_m: 0.7977 - precision_m: 0.7977 - recall_m: 0.7977 - val_loss: 0.4131 - val_acc: 0.6371 - val_f1_m: 0.6371 - val_precision_m: 0.6371 - val_recall_m: 0.6371\n",
      "Epoch 120/200\n",
      "45300/45300 [==============================] - 22s 483us/step - loss: 0.2289 - acc: 0.7992 - f1_m: 0.7992 - precision_m: 0.7992 - recall_m: 0.7992 - val_loss: 0.4143 - val_acc: 0.6302 - val_f1_m: 0.6302 - val_precision_m: 0.6302 - val_recall_m: 0.6302\n",
      "Epoch 121/200\n",
      "45300/45300 [==============================] - 22s 482us/step - loss: 0.2314 - acc: 0.7975 - f1_m: 0.7975 - precision_m: 0.7975 - recall_m: 0.7975 - val_loss: 0.4375 - val_acc: 0.6384 - val_f1_m: 0.6384 - val_precision_m: 0.6384 - val_recall_m: 0.6384\n",
      "Epoch 122/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2337 - acc: 0.7979 - f1_m: 0.7979 - precision_m: 0.7979 - recall_m: 0.7979 - val_loss: 0.4193 - val_acc: 0.6360 - val_f1_m: 0.6360 - val_precision_m: 0.6360 - val_recall_m: 0.6360\n",
      "Epoch 123/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.2326 - acc: 0.7968 - f1_m: 0.7968 - precision_m: 0.7968 - recall_m: 0.7968 - val_loss: 0.4259 - val_acc: 0.6291 - val_f1_m: 0.6291 - val_precision_m: 0.6291 - val_recall_m: 0.6291\n",
      "Epoch 124/200\n",
      "45300/45300 [==============================] - 22s 488us/step - loss: 0.2320 - acc: 0.7964 - f1_m: 0.7964 - precision_m: 0.7964 - recall_m: 0.7964 - val_loss: 0.4371 - val_acc: 0.6340 - val_f1_m: 0.6340 - val_precision_m: 0.6340 - val_recall_m: 0.6340\n",
      "Epoch 125/200\n",
      "45300/45300 [==============================] - 22s 483us/step - loss: 0.2378 - acc: 0.7932 - f1_m: 0.7932 - precision_m: 0.7932 - recall_m: 0.7932 - val_loss: 0.4278 - val_acc: 0.6311 - val_f1_m: 0.6311 - val_precision_m: 0.6311 - val_recall_m: 0.6311\n",
      "Epoch 126/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2300 - acc: 0.7981 - f1_m: 0.7981 - precision_m: 0.7981 - recall_m: 0.7981 - val_loss: 0.4325 - val_acc: 0.6230 - val_f1_m: 0.6230 - val_precision_m: 0.6230 - val_recall_m: 0.6230\n",
      "Epoch 127/200\n",
      "45300/45300 [==============================] - 22s 484us/step - loss: 0.2323 - acc: 0.7960 - f1_m: 0.7960 - precision_m: 0.7960 - recall_m: 0.7960 - val_loss: 0.4223 - val_acc: 0.6252 - val_f1_m: 0.6252 - val_precision_m: 0.6252 - val_recall_m: 0.6252\n",
      "Epoch 128/200\n",
      "45300/45300 [==============================] - 22s 482us/step - loss: 0.2334 - acc: 0.7945 - f1_m: 0.7944 - precision_m: 0.7945 - recall_m: 0.7944 - val_loss: 0.4226 - val_acc: 0.6249 - val_f1_m: 0.6249 - val_precision_m: 0.6249 - val_recall_m: 0.6249\n",
      "Epoch 129/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.2360 - acc: 0.7911 - f1_m: 0.7911 - precision_m: 0.7911 - recall_m: 0.7911 - val_loss: 0.4223 - val_acc: 0.6227 - val_f1_m: 0.6227 - val_precision_m: 0.6227 - val_recall_m: 0.6227\n",
      "Epoch 130/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2286 - acc: 0.7975 - f1_m: 0.7975 - precision_m: 0.7975 - recall_m: 0.7975 - val_loss: 0.4185 - val_acc: 0.6344 - val_f1_m: 0.6344 - val_precision_m: 0.6344 - val_recall_m: 0.6344\n",
      "Epoch 131/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.2288 - acc: 0.7978 - f1_m: 0.7978 - precision_m: 0.7978 - recall_m: 0.7978 - val_loss: 0.4196 - val_acc: 0.6349 - val_f1_m: 0.6349 - val_precision_m: 0.6349 - val_recall_m: 0.6349\n",
      "Epoch 132/200\n",
      "45300/45300 [==============================] - 22s 483us/step - loss: 0.2272 - acc: 0.8010 - f1_m: 0.8010 - precision_m: 0.8010 - recall_m: 0.8010 - val_loss: 0.4208 - val_acc: 0.6406 - val_f1_m: 0.6406 - val_precision_m: 0.6406 - val_recall_m: 0.6406\n",
      "Epoch 133/200\n",
      "45300/45300 [==============================] - 22s 477us/step - loss: 0.2300 - acc: 0.7981 - f1_m: 0.7981 - precision_m: 0.7981 - recall_m: 0.7981 - val_loss: 0.4297 - val_acc: 0.6285 - val_f1_m: 0.6285 - val_precision_m: 0.6285 - val_recall_m: 0.6285\n",
      "Epoch 134/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2311 - acc: 0.7988 - f1_m: 0.7988 - precision_m: 0.7988 - recall_m: 0.7988 - val_loss: 0.4258 - val_acc: 0.6291 - val_f1_m: 0.6291 - val_precision_m: 0.6291 - val_recall_m: 0.6291\n",
      "Epoch 135/200\n",
      "45300/45300 [==============================] - 22s 483us/step - loss: 0.2318 - acc: 0.7975 - f1_m: 0.7975 - precision_m: 0.7975 - recall_m: 0.7975 - val_loss: 0.4233 - val_acc: 0.6325 - val_f1_m: 0.6325 - val_precision_m: 0.6325 - val_recall_m: 0.6325\n",
      "Epoch 136/200\n",
      "45300/45300 [==============================] - 22s 484us/step - loss: 0.2316 - acc: 0.7955 - f1_m: 0.7955 - precision_m: 0.7955 - recall_m: 0.7955 - val_loss: 0.4301 - val_acc: 0.6291 - val_f1_m: 0.6291 - val_precision_m: 0.6291 - val_recall_m: 0.6291\n",
      "Epoch 137/200\n",
      "45300/45300 [==============================] - 22s 494us/step - loss: 0.2307 - acc: 0.7959 - f1_m: 0.7959 - precision_m: 0.7959 - recall_m: 0.7959 - val_loss: 0.4181 - val_acc: 0.6342 - val_f1_m: 0.6342 - val_precision_m: 0.6342 - val_recall_m: 0.6342\n",
      "Epoch 138/200\n",
      "45300/45300 [==============================] - 22s 491us/step - loss: 0.2231 - acc: 0.8013 - f1_m: 0.8013 - precision_m: 0.8013 - recall_m: 0.8013 - val_loss: 0.4304 - val_acc: 0.6331 - val_f1_m: 0.6331 - val_precision_m: 0.6331 - val_recall_m: 0.6331\n",
      "Epoch 139/200\n",
      "45300/45300 [==============================] - 22s 482us/step - loss: 0.2279 - acc: 0.8002 - f1_m: 0.8002 - precision_m: 0.8002 - recall_m: 0.8002 - val_loss: 0.4230 - val_acc: 0.6358 - val_f1_m: 0.6358 - val_precision_m: 0.6358 - val_recall_m: 0.6358\n",
      "Epoch 140/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2283 - acc: 0.8010 - f1_m: 0.8010 - precision_m: 0.8010 - recall_m: 0.8010 - val_loss: 0.4301 - val_acc: 0.6364 - val_f1_m: 0.6364 - val_precision_m: 0.6364 - val_recall_m: 0.6364\n",
      "Epoch 141/200\n",
      "45300/45300 [==============================] - 22s 477us/step - loss: 0.2330 - acc: 0.7991 - f1_m: 0.7991 - precision_m: 0.7991 - recall_m: 0.7991 - val_loss: 0.4409 - val_acc: 0.6351 - val_f1_m: 0.6351 - val_precision_m: 0.6351 - val_recall_m: 0.6351\n",
      "Epoch 142/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2297 - acc: 0.8000 - f1_m: 0.8000 - precision_m: 0.8000 - recall_m: 0.8000 - val_loss: 0.4258 - val_acc: 0.6285 - val_f1_m: 0.6285 - val_precision_m: 0.6285 - val_recall_m: 0.6285\n",
      "Epoch 143/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.2283 - acc: 0.7996 - f1_m: 0.7997 - precision_m: 0.7997 - recall_m: 0.7996 - val_loss: 0.4270 - val_acc: 0.6322 - val_f1_m: 0.6322 - val_precision_m: 0.6322 - val_recall_m: 0.6322\n",
      "Epoch 144/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.2290 - acc: 0.8000 - f1_m: 0.8000 - precision_m: 0.8000 - recall_m: 0.8000 - val_loss: 0.4252 - val_acc: 0.6214 - val_f1_m: 0.6214 - val_precision_m: 0.6214 - val_recall_m: 0.6214\n",
      "Epoch 145/200\n",
      "45300/45300 [==============================] - 22s 476us/step - loss: 0.2267 - acc: 0.8024 - f1_m: 0.8024 - precision_m: 0.8024 - recall_m: 0.8024 - val_loss: 0.4296 - val_acc: 0.6329 - val_f1_m: 0.6329 - val_precision_m: 0.6329 - val_recall_m: 0.6329\n",
      "Epoch 146/200\n",
      "45300/45300 [==============================] - 22s 483us/step - loss: 0.2252 - acc: 0.8031 - f1_m: 0.8031 - precision_m: 0.8031 - recall_m: 0.8031 - val_loss: 0.4217 - val_acc: 0.6373 - val_f1_m: 0.6373 - val_precision_m: 0.6373 - val_recall_m: 0.6373\n",
      "Epoch 147/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.2222 - acc: 0.8032 - f1_m: 0.8033 - precision_m: 0.8033 - recall_m: 0.8032 - val_loss: 0.4224 - val_acc: 0.6263 - val_f1_m: 0.6263 - val_precision_m: 0.6263 - val_recall_m: 0.6263\n",
      "Epoch 148/200\n",
      "45300/45300 [==============================] - 22s 477us/step - loss: 0.2291 - acc: 0.7984 - f1_m: 0.7984 - precision_m: 0.7984 - recall_m: 0.7984 - val_loss: 0.4217 - val_acc: 0.6300 - val_f1_m: 0.6300 - val_precision_m: 0.6300 - val_recall_m: 0.6300\n",
      "Epoch 149/200\n",
      "45300/45300 [==============================] - 22s 486us/step - loss: 0.2258 - acc: 0.8015 - f1_m: 0.8015 - precision_m: 0.8015 - recall_m: 0.8015 - val_loss: 0.4380 - val_acc: 0.6280 - val_f1_m: 0.6280 - val_precision_m: 0.6280 - val_recall_m: 0.6280\n",
      "Epoch 150/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.2218 - acc: 0.8051 - f1_m: 0.8052 - precision_m: 0.8052 - recall_m: 0.8051 - val_loss: 0.4337 - val_acc: 0.6241 - val_f1_m: 0.6241 - val_precision_m: 0.6241 - val_recall_m: 0.6241\n",
      "Epoch 151/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.2232 - acc: 0.8044 - f1_m: 0.8044 - precision_m: 0.8045 - recall_m: 0.8044 - val_loss: 0.4341 - val_acc: 0.6249 - val_f1_m: 0.6249 - val_precision_m: 0.6249 - val_recall_m: 0.6249\n",
      "Epoch 152/200\n",
      "45300/45300 [==============================] - 22s 476us/step - loss: 0.2254 - acc: 0.8037 - f1_m: 0.8037 - precision_m: 0.8037 - recall_m: 0.8037 - val_loss: 0.4241 - val_acc: 0.6260 - val_f1_m: 0.6260 - val_precision_m: 0.6260 - val_recall_m: 0.6260\n",
      "Epoch 153/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2258 - acc: 0.8021 - f1_m: 0.8021 - precision_m: 0.8021 - recall_m: 0.8021 - val_loss: 0.4226 - val_acc: 0.6358 - val_f1_m: 0.6358 - val_precision_m: 0.6358 - val_recall_m: 0.6358\n",
      "Epoch 154/200\n",
      "45300/45300 [==============================] - 22s 482us/step - loss: 0.2229 - acc: 0.8050 - f1_m: 0.8050 - precision_m: 0.8050 - recall_m: 0.8050 - val_loss: 0.4208 - val_acc: 0.6331 - val_f1_m: 0.6331 - val_precision_m: 0.6331 - val_recall_m: 0.6331\n",
      "Epoch 155/200\n",
      "45300/45300 [==============================] - 22s 477us/step - loss: 0.2211 - acc: 0.8065 - f1_m: 0.8065 - precision_m: 0.8065 - recall_m: 0.8065 - val_loss: 0.4229 - val_acc: 0.6265 - val_f1_m: 0.6265 - val_precision_m: 0.6265 - val_recall_m: 0.6265\n",
      "Epoch 156/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.2264 - acc: 0.8028 - f1_m: 0.8028 - precision_m: 0.8028 - recall_m: 0.8028 - val_loss: 0.4248 - val_acc: 0.6289 - val_f1_m: 0.6289 - val_precision_m: 0.6289 - val_recall_m: 0.6289\n",
      "Epoch 157/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2249 - acc: 0.8043 - f1_m: 0.8043 - precision_m: 0.8043 - recall_m: 0.8043 - val_loss: 0.4267 - val_acc: 0.6318 - val_f1_m: 0.6318 - val_precision_m: 0.6318 - val_recall_m: 0.6318\n",
      "Epoch 158/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.2262 - acc: 0.8021 - f1_m: 0.8021 - precision_m: 0.8021 - recall_m: 0.8021 - val_loss: 0.4299 - val_acc: 0.6208 - val_f1_m: 0.6208 - val_precision_m: 0.6208 - val_recall_m: 0.6208\n",
      "Epoch 159/200\n",
      "45300/45300 [==============================] - 21s 474us/step - loss: 0.2271 - acc: 0.8011 - f1_m: 0.8011 - precision_m: 0.8011 - recall_m: 0.8011 - val_loss: 0.4313 - val_acc: 0.6351 - val_f1_m: 0.6351 - val_precision_m: 0.6351 - val_recall_m: 0.6351\n",
      "Epoch 160/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.2287 - acc: 0.8011 - f1_m: 0.8011 - precision_m: 0.8011 - recall_m: 0.8011 - val_loss: 0.4340 - val_acc: 0.6338 - val_f1_m: 0.6338 - val_precision_m: 0.6338 - val_recall_m: 0.6338\n",
      "Epoch 161/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.2288 - acc: 0.8018 - f1_m: 0.8018 - precision_m: 0.8018 - recall_m: 0.8018 - val_loss: 0.4307 - val_acc: 0.6267 - val_f1_m: 0.6267 - val_precision_m: 0.6267 - val_recall_m: 0.6267\n",
      "Epoch 162/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2243 - acc: 0.8028 - f1_m: 0.8029 - precision_m: 0.8029 - recall_m: 0.8028 - val_loss: 0.4212 - val_acc: 0.6287 - val_f1_m: 0.6287 - val_precision_m: 0.6287 - val_recall_m: 0.6287\n",
      "Epoch 163/200\n",
      "45300/45300 [==============================] - 22s 485us/step - loss: 0.2281 - acc: 0.8026 - f1_m: 0.8025 - precision_m: 0.8026 - recall_m: 0.8025 - val_loss: 0.4317 - val_acc: 0.6309 - val_f1_m: 0.6309 - val_precision_m: 0.6309 - val_recall_m: 0.6309\n",
      "Epoch 164/200\n",
      "45300/45300 [==============================] - 22s 477us/step - loss: 0.2259 - acc: 0.8020 - f1_m: 0.8020 - precision_m: 0.8020 - recall_m: 0.8020 - val_loss: 0.4246 - val_acc: 0.6360 - val_f1_m: 0.6360 - val_precision_m: 0.6360 - val_recall_m: 0.6360\n",
      "Epoch 165/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2244 - acc: 0.8045 - f1_m: 0.8045 - precision_m: 0.8045 - recall_m: 0.8045 - val_loss: 0.4183 - val_acc: 0.6366 - val_f1_m: 0.6366 - val_precision_m: 0.6366 - val_recall_m: 0.6366\n",
      "Epoch 166/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.2276 - acc: 0.8008 - f1_m: 0.8008 - precision_m: 0.8008 - recall_m: 0.8008 - val_loss: 0.4178 - val_acc: 0.6366 - val_f1_m: 0.6366 - val_precision_m: 0.6366 - val_recall_m: 0.6366\n",
      "Epoch 167/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.2210 - acc: 0.8051 - f1_m: 0.8051 - precision_m: 0.8051 - recall_m: 0.8051 - val_loss: 0.4227 - val_acc: 0.6325 - val_f1_m: 0.6325 - val_precision_m: 0.6325 - val_recall_m: 0.6325\n",
      "Epoch 168/200\n",
      "45300/45300 [==============================] - 22s 486us/step - loss: 0.2218 - acc: 0.8055 - f1_m: 0.8055 - precision_m: 0.8055 - recall_m: 0.8055 - val_loss: 0.4222 - val_acc: 0.6331 - val_f1_m: 0.6331 - val_precision_m: 0.6331 - val_recall_m: 0.6331\n",
      "Epoch 169/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2207 - acc: 0.8040 - f1_m: 0.8040 - precision_m: 0.8040 - recall_m: 0.8040 - val_loss: 0.4316 - val_acc: 0.6311 - val_f1_m: 0.6311 - val_precision_m: 0.6311 - val_recall_m: 0.6311\n",
      "Epoch 170/200\n",
      "45300/45300 [==============================] - 22s 482us/step - loss: 0.2234 - acc: 0.8024 - f1_m: 0.8024 - precision_m: 0.8024 - recall_m: 0.8024 - val_loss: 0.4360 - val_acc: 0.6188 - val_f1_m: 0.6188 - val_precision_m: 0.6188 - val_recall_m: 0.6188\n",
      "Epoch 171/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2213 - acc: 0.8059 - f1_m: 0.8059 - precision_m: 0.8059 - recall_m: 0.8059 - val_loss: 0.4189 - val_acc: 0.6278 - val_f1_m: 0.6278 - val_precision_m: 0.6278 - val_recall_m: 0.6278\n",
      "Epoch 172/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2272 - acc: 0.8014 - f1_m: 0.8015 - precision_m: 0.8015 - recall_m: 0.8014 - val_loss: 0.4218 - val_acc: 0.6243 - val_f1_m: 0.6243 - val_precision_m: 0.6243 - val_recall_m: 0.6243\n",
      "Epoch 173/200\n",
      "45300/45300 [==============================] - 22s 476us/step - loss: 0.2269 - acc: 0.8014 - f1_m: 0.8014 - precision_m: 0.8014 - recall_m: 0.8014 - val_loss: 0.4266 - val_acc: 0.6245 - val_f1_m: 0.6245 - val_precision_m: 0.6245 - val_recall_m: 0.6245\n",
      "Epoch 174/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.2242 - acc: 0.8031 - f1_m: 0.8031 - precision_m: 0.8031 - recall_m: 0.8031 - val_loss: 0.4405 - val_acc: 0.6132 - val_f1_m: 0.6132 - val_precision_m: 0.6132 - val_recall_m: 0.6132\n",
      "Epoch 175/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2254 - acc: 0.8013 - f1_m: 0.8013 - precision_m: 0.8013 - recall_m: 0.8013 - val_loss: 0.4156 - val_acc: 0.6307 - val_f1_m: 0.6307 - val_precision_m: 0.6307 - val_recall_m: 0.6307\n",
      "Epoch 176/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.2240 - acc: 0.8028 - f1_m: 0.8028 - precision_m: 0.8028 - recall_m: 0.8027 - val_loss: 0.4322 - val_acc: 0.6307 - val_f1_m: 0.6307 - val_precision_m: 0.6307 - val_recall_m: 0.6307\n",
      "Epoch 177/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.2269 - acc: 0.8026 - f1_m: 0.8026 - precision_m: 0.8026 - recall_m: 0.8026 - val_loss: 0.4317 - val_acc: 0.6245 - val_f1_m: 0.6245 - val_precision_m: 0.6245 - val_recall_m: 0.6245\n",
      "Epoch 178/200\n",
      "45300/45300 [==============================] - 22s 483us/step - loss: 0.2237 - acc: 0.8048 - f1_m: 0.8048 - precision_m: 0.8048 - recall_m: 0.8048 - val_loss: 0.4271 - val_acc: 0.6289 - val_f1_m: 0.6289 - val_precision_m: 0.6289 - val_recall_m: 0.6289\n",
      "Epoch 179/200\n",
      "45300/45300 [==============================] - 22s 480us/step - loss: 0.2258 - acc: 0.8051 - f1_m: 0.8051 - precision_m: 0.8051 - recall_m: 0.8051 - val_loss: 0.4293 - val_acc: 0.6302 - val_f1_m: 0.6302 - val_precision_m: 0.6302 - val_recall_m: 0.6302\n",
      "Epoch 180/200\n",
      "45300/45300 [==============================] - 22s 482us/step - loss: 0.2233 - acc: 0.8056 - f1_m: 0.8056 - precision_m: 0.8056 - recall_m: 0.8056 - val_loss: 0.4294 - val_acc: 0.6263 - val_f1_m: 0.6263 - val_precision_m: 0.6263 - val_recall_m: 0.6263\n",
      "Epoch 181/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.2181 - acc: 0.8079 - f1_m: 0.8079 - precision_m: 0.8079 - recall_m: 0.8079 - val_loss: 0.4302 - val_acc: 0.6300 - val_f1_m: 0.6300 - val_precision_m: 0.6300 - val_recall_m: 0.6300\n",
      "Epoch 182/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2202 - acc: 0.8088 - f1_m: 0.8088 - precision_m: 0.8088 - recall_m: 0.8088 - val_loss: 0.4305 - val_acc: 0.6280 - val_f1_m: 0.6280 - val_precision_m: 0.6280 - val_recall_m: 0.6280\n",
      "Epoch 183/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.2249 - acc: 0.8045 - f1_m: 0.8045 - precision_m: 0.8045 - recall_m: 0.8045 - val_loss: 0.4266 - val_acc: 0.6325 - val_f1_m: 0.6325 - val_precision_m: 0.6325 - val_recall_m: 0.6325\n",
      "Epoch 184/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.2273 - acc: 0.8023 - f1_m: 0.8023 - precision_m: 0.8023 - recall_m: 0.8023 - val_loss: 0.4298 - val_acc: 0.6325 - val_f1_m: 0.6325 - val_precision_m: 0.6325 - val_recall_m: 0.6325\n",
      "Epoch 185/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.2214 - acc: 0.8062 - f1_m: 0.8062 - precision_m: 0.8062 - recall_m: 0.8062 - val_loss: 0.4375 - val_acc: 0.6287 - val_f1_m: 0.6287 - val_precision_m: 0.6287 - val_recall_m: 0.6287\n",
      "Epoch 186/200\n",
      "45300/45300 [==============================] - 22s 477us/step - loss: 0.2233 - acc: 0.8051 - f1_m: 0.8051 - precision_m: 0.8051 - recall_m: 0.8051 - val_loss: 0.4287 - val_acc: 0.6313 - val_f1_m: 0.6313 - val_precision_m: 0.6313 - val_recall_m: 0.6313\n",
      "Epoch 187/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2242 - acc: 0.8043 - f1_m: 0.8043 - precision_m: 0.8043 - recall_m: 0.8043 - val_loss: 0.4327 - val_acc: 0.6199 - val_f1_m: 0.6199 - val_precision_m: 0.6199 - val_recall_m: 0.6199\n",
      "Epoch 188/200\n",
      "45300/45300 [==============================] - 21s 474us/step - loss: 0.2209 - acc: 0.8055 - f1_m: 0.8054 - precision_m: 0.8055 - recall_m: 0.8054 - val_loss: 0.4294 - val_acc: 0.6260 - val_f1_m: 0.6260 - val_precision_m: 0.6260 - val_recall_m: 0.6260\n",
      "Epoch 189/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.2219 - acc: 0.8054 - f1_m: 0.8054 - precision_m: 0.8055 - recall_m: 0.8054 - val_loss: 0.4395 - val_acc: 0.6166 - val_f1_m: 0.6166 - val_precision_m: 0.6166 - val_recall_m: 0.6166\n",
      "Epoch 190/200\n",
      "45300/45300 [==============================] - 22s 477us/step - loss: 0.2219 - acc: 0.8054 - f1_m: 0.8054 - precision_m: 0.8054 - recall_m: 0.8054 - val_loss: 0.4340 - val_acc: 0.6276 - val_f1_m: 0.6276 - val_precision_m: 0.6276 - val_recall_m: 0.6276\n",
      "Epoch 191/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2247 - acc: 0.8039 - f1_m: 0.8039 - precision_m: 0.8039 - recall_m: 0.8039 - val_loss: 0.4330 - val_acc: 0.6309 - val_f1_m: 0.6309 - val_precision_m: 0.6309 - val_recall_m: 0.6309\n",
      "Epoch 192/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.2238 - acc: 0.8035 - f1_m: 0.8035 - precision_m: 0.8035 - recall_m: 0.8035 - val_loss: 0.4312 - val_acc: 0.6311 - val_f1_m: 0.6311 - val_precision_m: 0.6311 - val_recall_m: 0.6311\n",
      "Epoch 193/200\n",
      "45300/45300 [==============================] - 22s 479us/step - loss: 0.2214 - acc: 0.8045 - f1_m: 0.8045 - precision_m: 0.8045 - recall_m: 0.8045 - val_loss: 0.4296 - val_acc: 0.6212 - val_f1_m: 0.6213 - val_precision_m: 0.6214 - val_recall_m: 0.6212\n",
      "Epoch 194/200\n",
      "45300/45300 [==============================] - 22s 482us/step - loss: 0.2219 - acc: 0.8053 - f1_m: 0.8053 - precision_m: 0.8053 - recall_m: 0.8053 - val_loss: 0.4319 - val_acc: 0.6203 - val_f1_m: 0.6202 - val_precision_m: 0.6202 - val_recall_m: 0.6201\n",
      "Epoch 195/200\n",
      "45300/45300 [==============================] - 22s 476us/step - loss: 0.2274 - acc: 0.8023 - f1_m: 0.8023 - precision_m: 0.8023 - recall_m: 0.8023 - val_loss: 0.4427 - val_acc: 0.6278 - val_f1_m: 0.6278 - val_precision_m: 0.6278 - val_recall_m: 0.6278\n",
      "Epoch 196/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.2248 - acc: 0.8037 - f1_m: 0.8037 - precision_m: 0.8037 - recall_m: 0.8037 - val_loss: 0.4403 - val_acc: 0.6291 - val_f1_m: 0.6291 - val_precision_m: 0.6291 - val_recall_m: 0.6291\n",
      "Epoch 197/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.2203 - acc: 0.8054 - f1_m: 0.8054 - precision_m: 0.8054 - recall_m: 0.8054 - val_loss: 0.4294 - val_acc: 0.6280 - val_f1_m: 0.6281 - val_precision_m: 0.6282 - val_recall_m: 0.6280\n",
      "Epoch 198/200\n",
      "45300/45300 [==============================] - 22s 481us/step - loss: 0.2203 - acc: 0.8078 - f1_m: 0.8078 - precision_m: 0.8078 - recall_m: 0.8077 - val_loss: 0.4330 - val_acc: 0.6298 - val_f1_m: 0.6298 - val_precision_m: 0.6298 - val_recall_m: 0.6298\n",
      "Epoch 199/200\n",
      "45300/45300 [==============================] - 22s 476us/step - loss: 0.2206 - acc: 0.8071 - f1_m: 0.8071 - precision_m: 0.8071 - recall_m: 0.8071 - val_loss: 0.4339 - val_acc: 0.6336 - val_f1_m: 0.6336 - val_precision_m: 0.6336 - val_recall_m: 0.6336\n",
      "Epoch 200/200\n",
      "45300/45300 [==============================] - 22s 478us/step - loss: 0.2217 - acc: 0.8065 - f1_m: 0.8065 - precision_m: 0.8065 - recall_m: 0.8065 - val_loss: 0.4308 - val_acc: 0.6344 - val_f1_m: 0.6344 - val_precision_m: 0.6344 - val_recall_m: 0.6344\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 200\n",
    "\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint('saved_model.h5', verbose=0, monitor='val_loss',save_best_only=True, mode='min')\n",
    "reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=0, epsilon=1e-4, mode='min')\n",
    "\n",
    "\n",
    "history = mdl.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=batch_size, epochs=num_epochs, \n",
    "          callbacks=[\n",
    "                     #earlyStopping, \n",
    "                     mcp_save]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "colab_type": "code",
    "id": "b5is5hZTgFsT",
    "outputId": "e5db7910-c4b3-4183-cbf8-be7a08e6e773"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmwAAAGuCAYAAAAtXVoHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(history.epoch,history.history['loss'],history.epoch,history.history['val_loss'], linewidth=2)\n",
    "# plt.plot(history.epoch,history.history['f1_m'],history.epoch,history.history['val_f1_m'])\n",
    "plt.ylabel('Loss', fontsize=15)\n",
    "plt.xlabel('Epoch', fontsize=15)\n",
    "plt.legend(['Training loss', 'Validation loss'], fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "pJgjbdWACfyF",
    "outputId": "ddebb0b1-b96a-4764-abdf-ae72349c69df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504/504 [==============================] - 1s 3ms/step\n",
      "loss: 0.3819, accuracy: 0.6448, f1-score: 0.6448, precision: 0.6448, recall: 0.6448\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "mdl = load_model('saved_model.h5', custom_objects={\n",
    "                                                  'f1_loss':f1_loss, \n",
    "                                                  'f1_m':f1_m,\n",
    "                                                  'precision_m':precision_m,\n",
    "                                                  'recall_m':recall_m\n",
    "                                                   })\n",
    "loss, acc, f1, prec, rec = mdl.evaluate(X_test, y_test)\n",
    "print(\"loss: {}, accuracy: {}, f1-score: {}, precision: {}, recall: {}\".format(round(loss,4), round(acc,4), round(f1,4), round(prec,4), round(rec,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ONYwJPOvOQHe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,3]+[4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'a':[1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    a['b']\n",
    "except KeyError:\n",
    "    print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from w2vLSTM import score_CNN_LSTM as m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, int found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a0267764728c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mm2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\python ipynbs\\Leiden University\\Text Mining\\project\\sentimentAnalysis\\w2vLSTM.py\u001b[0m in \u001b[0;36mscore_CNN_LSTM\u001b[1;34m(X_train, y_train, X_val, y_val, X_test, y_test, min_count, epochs, batch_size, embedding_size, dropout, verbose)\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;31m# BB_twtr at SemEval-2017 Task 4: Twitter Sentiment Analysis with CNNs and LSTMs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m     \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m     \u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 0: expected str instance, int found"
     ]
    }
   ],
   "source": [
    "m2([1],[0],[1],[0],[1],[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fwalo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word 'asd' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-7a0a17cf9607>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'asd sjanfk'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'asdsf kjasn'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'asfans aslknd'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mwv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'<$>'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-7a0a17cf9607>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'asd sjanfk'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'asdsf kjasn'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'asfans aslknd'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mwv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'<$>'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1445\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1446\u001b[0m                 )\n\u001b[1;32m-> 1447\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1449\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, words)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1120\u001b[0m         \"\"\"\n\u001b[1;32m-> 1121\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Method will be removed in 4.0.0, use self.wv.__contains__() instead\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, entities)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[1;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'asd' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "X_train = ['asd sjanfk','asdsf kjasn','asfans aslknd']\n",
    "wv = Word2Vec(\" \".join(X_train).split(\" \")+['<$>'])\n",
    "[wv[word] for word in \" \".join(X_train).split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asd', 'asdsf', 'asfans', ['<$>'], ['<$>'], ['<$>']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.append(['<$>'])\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "TM_Assignment_4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
